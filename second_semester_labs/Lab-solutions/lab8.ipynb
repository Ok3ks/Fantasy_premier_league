{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rural-gasoline",
   "metadata": {},
   "source": [
    "# Week 8: Using Transformer Models\n",
    "\n",
    "\n",
    "## Getting started\n",
    "If working on your own machine, make sure the huggingface transformers package is installed\n",
    "\n",
    "`conda install -c huggingface transformers`\n",
    "\n",
    "or\n",
    "\n",
    "`pip install transformers`\n",
    "\n",
    "Of course, if working on Google Colab, you won't need to do this.  Whatever environment you are using check whether the following code runs.  It should output a negative label with a high score!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "therapeutic-fireplace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9991129040718079}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "print(pipeline('sentiment-analysis')('I hate you'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-joshua",
   "metadata": {},
   "source": [
    "The following is adapted from the huggingface quickstart to transformers tutorial https://huggingface.co/transformers/quickstart.html\n",
    "We will be looking at the BERT introduction (but feel free to have a look at GPT2 etc as well!)\n",
    "\n",
    "First of all we need some key imports.  We are going to be using the pre-trained bert-base-uncased model so this cell instantiates a tokenizer for this model.  Logging is also switched on so we can see more of what's going on. The first time you run it, the model will be downloaded and cached.  The cached version will be used on subsequent runs, if it is available (not on Google CoLab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "certified-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as torch_mp\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da3802c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-lesbian",
   "metadata": {},
   "source": [
    "Now we are going to tokenize some text.  This will demonstrate the 'wordpiece' vocabulary used by BERT as well as the fact that we need to introduce special `[CLS]` and `[SEP]` tokens in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "buried-sponsorship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'who', 'was', 'elected', 'as', 'british', 'prime', 'minister', 'in', '1951', '?', '[SEP]', 'sir', 'winston', 'leonard', 'spencer', 'churchill', 'was', 'a', 'british', 'politician', ',', 'statesman', ',', 'army', 'officer', 'and', 'writer', ',', 'who', 'was', 'prime', 'minister', 'of', 'the', 'united', 'kingdom', 'from', '1940', 'to', '1945', 'and', 'again', 'from', '1951', 'to', '1955', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize input\n",
    "#use re.sub to replace . with [SEP], marking end of sentence.\n",
    "text = \"[CLS] Who was elected as British prime minister in 1951? [SEP] Sir Winston Leonard Spencer Churchill was a British politician, statesman, army officer and writer, who was Prime Minister of the United Kingdom from 1940 to 1945 and again from 1951 to 1955. [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "binding-diana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'what', 'are', 'ign', '##eous', 'rocks', '?', '[SEP]', 'ign', '##eous', 'rocks', 'form', 'when', 'hot', ',', 'molten', 'rock', 'crystal', '##li', '##zes', 'and', 'solid', '##ifies', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize input\n",
    "text = \"[CLS] What are igneous rocks? [SEP] Igneous rocks form when hot , molten rock crystallizes and solidifies. [SEP] \"\n",
    "tokenized_text= tokenizer.tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-dealing",
   "metadata": {},
   "source": [
    "Note that the tokenizer is not breaking down all words according to their morphology -- only rare words.  Reasonably frequent words such as `elected` are left as whole words.  Rarer words such as `solidifies` are broken down.\n",
    "\n",
    "Now we are going to mask out one of the words in the text.  For the purposes of this demonstration, I have chosen token 11 but you could try different tokens.  Remember that during training the tokens to mask are chosen randomly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "detailed-valuation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'what', 'are', 'ign', '##eous', 'rocks', '?', '[SEP]', 'ign', '##eous', 'rocks', '[MASK]', 'when', 'hot', ',', 'molten', 'rock', 'crystal', '##li', '##zes', 'and', 'solid', '##ifies', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 11\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dressed-forwarding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-completion",
   "metadata": {},
   "source": [
    "We are now going to try to use the masked language model to predict this word.\n",
    "\n",
    "First we need to convert the input into a list of word index ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "circular-playing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2054, 2024, 16270, 14769, 5749, 1029, 102, 16270, 14769, 5749, 103, 2043, 2980, 1010, 23548, 2600, 6121, 3669, 11254, 1998, 5024, 14144, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "print(indexed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-papua",
   "metadata": {},
   "source": [
    "We need segment ids to define whether a token is in the first or second sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pointed-croatia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def make_segment_ids(list_of_tokens):\n",
    "    #this function assumes that up to and including the first '[SEP]' is the first segment, anything afterwards is the second segment\n",
    "    current_id=0\n",
    "    segment_ids=[]\n",
    "    for token in list_of_tokens:\n",
    "        segment_ids.append(current_id)\n",
    "        if token == '[SEP]':\n",
    "            current_id +=1\n",
    "    return segment_ids\n",
    "\n",
    "segment_ids=make_segment_ids(tokenized_text)\n",
    "print(segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "recorded-bundle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2054,  2024, 16270, 14769,  5749,  1029,   102, 16270, 14769,\n",
      "          5749,   103,  2043,  2980,  1010, 23548,  2600,  6121,  3669, 11254,\n",
      "          1998,  5024, 14144,  1012,   102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]])\n"
     ]
    }
   ],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "#this just wraps things up in multi-dimensional tensors rather than as flat lists.\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segment_ids])\n",
    "print(tokens_tensor)\n",
    "print(segments_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-motivation",
   "metadata": {},
   "source": [
    "Now we need to encode the input using the bert-base-uncased model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "present-apparatus",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set the model in evaluation mode to deactivate the DropOut modules\n",
    "# This is IMPORTANT to have reproducible results during evaluation!\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda - otherwise comment this out to run on CPU\n",
    "#tokens_tensor = tokens_tensor.to('cuda')\n",
    "#segments_tensors = segments_tensors.to('cuda')\n",
    "#model.to('cuda')\n",
    "\n",
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    # See the models docstrings for the detail of the inputs\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    # Transformers models always output tuples.\n",
    "    # See the models docstrings for the detail of all the outputs\n",
    "    # In our case, the first element of outputs is the output of the last layer of the Bert model (all tokens)\n",
    "    # the second element of outputs, outputs[1] is actually just a \"pooled_output\" representation of the CLS token (rather than all tokens) - however this involves an extra layer which is why it is not the same as the first element in outputs[0]!\n",
    "    encoded_layers = outputs[0]\n",
    "# We have encoded our input sequence in a FloatTensor of shape (batch size, sequence length, model hidden dimension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "powerful-today",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25, 768])\n"
     ]
    }
   ],
   "source": [
    "# We have encoded our input sequence in a FloatTensor of shape (batch size, sequence length, model hidden dimension)\n",
    "print(encoded_layers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ultimate-orlando",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6674,  0.9458, -0.5036,  ..., -0.5302,  0.1874,  0.6336],\n",
       "         [ 0.3067, -0.0335,  0.0011,  ...,  0.1267, -0.3640, -0.9253],\n",
       "         [ 0.4349,  0.3408,  0.5464,  ..., -0.4371, -0.1396,  0.6499],\n",
       "         ...,\n",
       "         [ 0.2870,  0.5662, -0.0956,  ..., -0.5547, -0.4900, -0.2980],\n",
       "         [ 0.6253,  0.0615, -0.2790,  ...,  0.0038, -0.5781, -0.4948],\n",
       "         [ 0.6399,  0.0483, -0.2593,  ...,  0.0093, -0.5864, -0.4718]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "complimentary-shoot",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#outputs[1] is a representation of the CLS token of shape (batch size, model hidden dimension)\n",
    "outputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "appropriate-demographic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9920, -0.8534, -0.9981,  0.9898,  0.9694, -0.7778,  0.9918,  0.7435,\n",
       "         -0.9951, -1.0000, -0.9537,  0.9990,  0.9948,  0.9125,  0.9905, -0.9797,\n",
       "         -0.9501, -0.9055,  0.7429, -0.9396,  0.9539,  1.0000, -0.7118,  0.8053,\n",
       "          0.8925,  1.0000, -0.9784,  0.9881,  0.9908,  0.8765, -0.9697,  0.7592,\n",
       "         -0.9981, -0.6847, -0.9970, -0.9994,  0.9093, -0.9470, -0.6564, -0.6737,\n",
       "         -0.9759,  0.8515,  1.0000,  0.4217,  0.8609, -0.7738, -1.0000,  0.7504,\n",
       "         -0.9733,  0.9991,  0.9951,  0.9958,  0.8245,  0.9274,  0.9106, -0.8983,\n",
       "          0.6171,  0.6578, -0.7522, -0.9329, -0.8695,  0.8373, -0.9920, -0.9770,\n",
       "          0.9985,  0.9901, -0.7740, -0.8020, -0.7395,  0.4467,  0.9907,  0.7470,\n",
       "         -0.6924, -0.9460,  0.9862,  0.7678, -0.8415,  1.0000, -0.9650, -0.9968,\n",
       "          0.9861,  0.9893,  0.8187, -0.9405,  0.9565, -1.0000,  0.9414, -0.6060,\n",
       "         -0.9974,  0.7915,  0.9118, -0.7666,  0.9441,  0.8653, -0.8785, -0.9049,\n",
       "         -0.8647, -0.9939, -0.7791, -0.8847,  0.6838, -0.7547, -0.9171, -0.7626,\n",
       "          0.7974, -0.9089, -0.9127,  0.9330,  0.8373,  0.9413,  0.7347, -0.7677,\n",
       "          0.9071, -0.9925,  0.9362, -0.7880, -0.9977, -0.8618, -0.9973,  0.8958,\n",
       "         -0.8342, -0.7956,  0.9952, -0.7976,  0.8478, -0.7864, -0.9984, -1.0000,\n",
       "         -0.9598, -0.9045, -0.7693, -0.8251, -0.9955, -0.9902,  0.9385,  0.9900,\n",
       "          0.7818,  1.0000, -0.8784,  0.9820, -0.8444, -0.9638,  0.9604, -0.8807,\n",
       "          0.9772,  0.8863, -0.9577,  0.7125, -0.9123,  0.8488, -0.9688, -0.8415,\n",
       "         -0.9844, -0.9808, -0.7662,  0.9912, -0.8856, -0.9991, -0.7581, -0.7853,\n",
       "         -0.8826,  0.9685,  0.9713,  0.8696, -0.9014,  0.8728,  0.8884,  0.9016,\n",
       "         -0.9784, -0.7636,  0.8841, -0.8225, -0.9879, -0.9964, -0.8620,  0.8534,\n",
       "          0.9980,  0.9479,  0.7873,  0.9910, -0.7581,  0.9552, -0.9899,  0.9966,\n",
       "         -0.7570,  0.5980, -0.7977,  0.8375, -0.9794,  0.5972,  0.9890, -0.9660,\n",
       "         -0.9679, -0.7693, -0.8489, -0.9049, -0.9854,  0.8821, -0.7518, -0.7079,\n",
       "         -0.6682,  0.9754,  0.9996,  0.9721,  0.9171,  0.9500, -0.9908, -0.8700,\n",
       "          0.7582,  0.7664,  0.7310,  0.9992, -0.9316, -0.7278, -0.9734, -0.9959,\n",
       "          0.6740, -0.9737, -0.7451, -0.9224,  0.9654, -0.8308,  0.9702,  0.8731,\n",
       "         -0.9997, -0.9499,  0.8576, -0.8373,  0.8823, -0.7450,  0.6808,  0.9977,\n",
       "         -0.9323,  0.9893,  0.9728, -0.9950, -0.9247,  0.9822, -0.8109,  0.9878,\n",
       "         -0.9496,  0.9998,  0.9973,  0.9920, -0.9888, -0.9745, -0.9904, -0.9865,\n",
       "         -0.7125,  0.8473,  0.9979,  0.8981,  0.9016, -0.5928, -0.9488,  1.0000,\n",
       "         -0.3508, -0.9907, -0.1536, -0.8569, -0.9974,  0.9897,  0.7493,  0.9048,\n",
       "         -0.8930, -0.9627, -0.9920,  0.9959,  0.7362,  0.9997, -0.8934, -0.9988,\n",
       "         -0.9553, -0.9907,  0.6157, -0.7970, -0.9377,  0.4954, -0.9907,  0.8796,\n",
       "          0.8464,  0.9228, -0.9960,  1.0000,  1.0000,  0.9947,  0.9719,  0.9923,\n",
       "         -1.0000, -0.6040,  1.0000, -1.0000, -1.0000, -0.9882, -0.9459,  0.7377,\n",
       "         -1.0000, -0.6635, -0.6586, -0.9673,  0.9837,  0.9952,  1.0000, -1.0000,\n",
       "          0.9634,  0.9909, -0.8944,  0.9994, -0.9058,  0.9945,  0.8854,  0.8465,\n",
       "         -0.7271,  0.8456, -0.9985, -0.9951, -0.9732, -0.9846,  1.0000,  0.6920,\n",
       "         -0.9485, -0.9853,  0.8946, -0.7972,  0.6410, -0.9914, -0.7770,  0.9767,\n",
       "          0.9624,  0.7348,  0.7770, -0.9365,  0.8198,  0.6665,  0.8833,  0.8675,\n",
       "         -0.9828, -0.8991, -0.8155,  0.6827, -0.9729, -0.9920,  0.9961, -0.8314,\n",
       "          0.9971,  1.0000,  0.8267, -0.9900,  0.9614,  0.8303, -0.5444,  1.0000,\n",
       "          0.9586, -0.9967, -0.8382,  0.9182, -0.9399, -0.9588,  1.0000, -0.7798,\n",
       "         -0.9867, -0.9522,  0.9948, -0.9978,  1.0000, -0.9881, -0.9905,  0.9944,\n",
       "          0.9871, -0.9434, -0.8677,  0.7637, -0.9775,  0.7947, -0.9945,  0.9808,\n",
       "          0.9346, -0.6588,  0.9742, -0.9940, -0.7931,  0.7875, -0.9799, -0.7599,\n",
       "          0.9980,  0.9043, -0.7754,  0.6658, -0.7986, -0.3307, -0.9923,  0.9022,\n",
       "          1.0000, -0.8227,  0.9825, -0.9080, -0.6014,  0.6226,  0.9159,  0.9415,\n",
       "         -0.7960, -0.9544,  0.9823, -0.9982, -0.9964,  0.9476,  0.7602, -0.8235,\n",
       "          1.0000,  0.9153,  0.7399,  0.8395,  1.0000,  0.6071,  0.9156,  0.9986,\n",
       "          0.9961, -0.7822,  0.8492,  0.9883, -0.9988, -0.8254, -0.9167,  0.6585,\n",
       "         -0.9827, -0.6974, -0.9918,  0.9918,  0.9995,  0.8936,  0.7949,  0.9690,\n",
       "          1.0000, -0.4737,  0.9129, -0.9544,  0.9918, -1.0000, -0.9814, -0.8310,\n",
       "         -0.7560, -0.9961, -0.7968,  0.7888, -0.9939,  0.9953,  0.9693, -0.9999,\n",
       "         -0.9981, -0.8581,  0.9874,  0.6808, -0.9999, -0.9777, -0.8569,  0.9685,\n",
       "         -0.8493, -0.9911, -0.9130, -0.8793,  0.8907, -0.7382,  0.7991,  0.9954,\n",
       "         -0.3901, -0.9847, -0.7989, -0.6627, -0.9784,  0.9864, -0.9725, -0.9983,\n",
       "         -0.7337,  1.0000, -0.7928,  0.9937,  0.9591,  0.9633, -0.7954,  0.6570,\n",
       "          0.9967,  0.8362, -0.9908, -0.9979, -0.9884, -0.8979,  0.9179,  0.9442,\n",
       "          0.9760,  0.9389,  0.9495,  0.7606, -0.6604,  0.7730,  1.0000, -0.7958,\n",
       "         -0.8235, -0.8818, -0.6877, -0.8061, -0.9523,  1.0000,  0.7959,  0.9194,\n",
       "         -0.9978, -0.9934, -0.9967,  1.0000,  0.9537, -0.9557,  0.9569,  0.9294,\n",
       "         -0.6975,  0.9865, -0.7497, -0.7547,  0.7697,  0.6506,  0.9939, -0.8970,\n",
       "         -0.9937, -0.8969,  0.8814, -0.9936,  1.0000, -0.9344, -0.6993, -0.7738,\n",
       "         -0.9229,  0.9756,  0.5358, -0.9948, -0.8095,  0.7254,  0.9943,  0.8051,\n",
       "         -0.8519, -0.9900,  0.9966,  0.9833, -0.9981, -0.9795,  0.9948, -0.9960,\n",
       "          0.9296,  1.0000,  0.7765,  0.9189,  0.8173, -0.8967,  0.8645, -0.8983,\n",
       "          0.9594, -0.9923, -0.8647, -0.7741,  0.8819, -0.8473, -0.8005,  0.9022,\n",
       "          0.7010, -0.7941, -0.9262, -0.8005,  0.8966,  0.9892, -0.7817, -0.7274,\n",
       "          0.7499, -0.7966, -0.9950, -0.8403, -0.9028, -1.0000,  0.9396, -1.0000,\n",
       "          0.9699,  0.8453, -0.6894,  0.9600,  0.5040,  0.9370, -0.9575, -0.9969,\n",
       "         -0.7215,  0.9566, -0.8462, -0.9478, -0.9579,  0.8933, -0.7940,  0.6971,\n",
       "         -0.9605,  0.9234, -0.8134,  1.0000,  0.7787, -0.9636, -0.9995,  0.7418,\n",
       "         -0.8454,  1.0000, -0.9968, -0.9925,  0.8237, -0.9792, -0.9584,  0.7873,\n",
       "          0.6013, -0.9759, -0.9990,  0.9966,  0.9955, -0.7298,  0.9346, -0.8549,\n",
       "         -0.9028,  0.5879,  0.9959,  0.9975,  0.9175,  0.9933, -0.5805, -0.7641,\n",
       "          0.9928,  0.8033,  0.9636,  0.6838,  1.0000,  0.8710, -0.9845, -0.5421,\n",
       "         -0.9976, -0.8036, -0.9960,  0.8349,  0.8067,  0.9849, -0.8317,  0.9972,\n",
       "         -0.9945,  0.6559, -0.9616, -0.9538,  0.8082, -0.9879, -0.9956, -0.9971,\n",
       "          0.9609, -0.8136, -0.6031,  0.7689,  0.7609,  0.9146,  0.8965, -1.0000,\n",
       "          0.9870,  0.9008,  0.9970,  0.9928,  0.9553,  0.9059,  0.7928, -0.9978,\n",
       "         -0.9997, -0.8499, -0.7567,  0.9731,  0.9451,  0.9759,  0.8792, -0.7774,\n",
       "         -0.8035, -0.9736, -0.6400, -0.9987,  0.8177, -0.9809, -0.9993,  0.9907,\n",
       "          0.5525, -0.6939, -0.7384, -0.9846,  0.9975,  0.9612,  0.8804,  0.6486,\n",
       "          0.8515,  0.9766,  0.9948,  0.9970, -0.9918,  0.9730, -0.9867,  0.8941,\n",
       "          0.5481, -0.9805,  0.7424,  0.9325, -0.8293,  0.7965, -0.7773, -0.9986,\n",
       "          0.5689, -0.7585,  0.9433, -0.8013, -0.6182, -0.8715, -0.6922, -0.9015,\n",
       "         -0.9589,  0.8264,  0.9391,  0.9861,  0.9824, -0.6870, -0.9766, -0.7581,\n",
       "         -0.9901, -0.9811,  0.9974, -0.7849, -0.9224,  0.9792,  0.6575,  0.7181,\n",
       "          0.7718, -0.8658, -0.8692, -0.9748,  0.9783, -0.7775, -0.9432, -0.9298,\n",
       "          0.9382,  0.8087,  1.0000, -0.9894, -0.9983, -0.8440, -0.8447,  0.7453,\n",
       "         -0.9005, -1.0000,  0.8348, -0.9474,  0.9618, -0.9782,  0.9927, -0.9642,\n",
       "         -0.9994, -0.8306,  0.8780,  0.9822, -0.8960, -0.9683,  0.8213, -0.9105,\n",
       "          0.9998,  0.9668, -0.9696, -0.6894,  0.8576, -0.9900, -0.9290,  0.9831]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-intelligence",
   "metadata": {},
   "source": [
    "We can also predict the masked token as follows.  We make the predictions as before (using the last layer of the BERT model) but then we find the token id which maximises the prediction for the masked token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "postal-genealogy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "form\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "#tokens_tensor = tokens_tensor.to('cuda')\n",
    "#segments_tensors = segments_tensors.to('cuda')\n",
    "#model.to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "        \n",
    "# find the token id which maximises the prediction for the masked token and then convert this back to a word\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "print(predicted_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-handbook",
   "metadata": {},
   "source": [
    "### Exercise 0\n",
    "Mask each token in turn and see what BERT predicts.   How accurate are its predictions?  As an extension, you could look at masking multiple words in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "capital-johnston",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use for loop to mask each token in turn\n",
    "#convert into tensors, use Bertformaskedlm\n",
    "#make prediction\n",
    "#Store masked token, predicted token\n",
    "#compute accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "331bcfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "def comp_accuracy(alist1,alist2):\n",
    "    \"\"\"Takes in two lists\"\"\"\n",
    "    incorrect = 0\n",
    "    if isinstance(alist1, list) and isinstance(alist2, list):\n",
    "        if len(alist1) == len(alist2):\n",
    "            for goldword,word in zip(alist1,alist2):\n",
    "                if goldword == word:\n",
    "                    pass\n",
    "                else: incorrect + 1\n",
    "        else: print(\"Lists must be of equal length\")\n",
    "\n",
    "    else: print(\"function takes in two lists\")\n",
    "    accuracy = ((len(alist1) - incorrect)/len(alist1))*100\n",
    "    return accuracy\n",
    "\n",
    "def predict_word(tok, model):\n",
    "    \"\"\"Takes in a list of tokenized corpus,tok, and model used for prediction\"\"\"\n",
    "    masked_words = []; predicted_words = []\n",
    "    for index,token in enumerate(tok):\n",
    "        hidden_index = index\n",
    "        tok[hidden_index] = '[MASK]' ; masked_words.append(token)\n",
    "        indexed_tokens_n = tokenizer.convert_tokens_to_ids(tok) \n",
    "        segmented_ids = make_segment_ids(tok)\n",
    "\n",
    "        token_tensor_n = torch.tensor([indexed_tokens_n])\n",
    "        segments_tensors_n = torch.tensor([segmented_ids])\n",
    "\n",
    "        #ONly forward gradient \n",
    "        with torch.no_grad():\n",
    "            outputs = model(token_tensor_n, token_type_ids = segments_tensors_n)\n",
    "            predictions = outputs[0]\n",
    "\n",
    "\n",
    "        predicted_index = torch.argmax(predictions[0, hidden_index]).item()\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "        predicted_words.append(predicted_token)\n",
    "    print(\"The accuracy of the Model - {} is {}\".format(str(model)[0:10],comp_accuracy(masked_words, predicted_words)))\n",
    "\n",
    "Bert_model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2084c468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Model - BertForMas is 100.0\n"
     ]
    }
   ],
   "source": [
    "predict_word(tokenized_text, Bert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-muslim",
   "metadata": {},
   "source": [
    "## Representing Sentential Meaning\n",
    "We are going to be looking at different strategies for representing sentential meaning\n",
    "* CLS token representation\n",
    "* centroid/sum of output embeddings\n",
    "\n",
    "The file `examples.txt` contains some example sentences.\n",
    "\n",
    "### Exercise 1\n",
    "Read in the sentences and store them as a list of sentences.  Add `[CLS]` and `[SEP]` tokens to the beginning and end of each and then pass them through the bert-base-uncased tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "collectible-large",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"/Users/max/Desktop/Nlp/resources/examples.txt\"\n",
    "content = []; temp_tokens = \"\" \n",
    "import re \n",
    "with open(input_file, 'r') as inp:\n",
    "    for line in inp.readlines():\n",
    "        line = re.sub(\"(.\\n)\",\" [SEP]\",line)\n",
    "        #temp_tokens = temp_tokens + \n",
    "        temp_tokens = '[CLS]' + \" \" + line\n",
    "        content.append(tokenizer.tokenize(temp_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-direction",
   "metadata": {},
   "source": [
    "When encoding sentences, it is actually more typical to pool the hidden states for each layer (at depth n) rather than the output layer.  We can access the hidden states of the model using `output_hidden_states=True` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[CLS]', 'the', 'boy', 'kicks', 'the', 'ball', '[SEP]'],\n",
       " ['[CLS]', 'the', 'ball', 'kicks', 'the', 'boy', '[SEP]'],\n",
       " ['[CLS]', 'the', 'child', 'kicks', 'the', 'ball', '[SEP]'],\n",
       " ['[CLS]', 'the', 'ball', 'is', 'kicked', 'by', 'the', 'boy', '[SEP]'],\n",
       " ['[CLS]', 'the', 'ball', 'is', 'kicked', '[SEP]'],\n",
       " ['[CLS]', 'the', 'boy', 'kicks', '[SEP]'],\n",
       " ['[CLS]', 'the', 'child', 'kicks', '[SEP]'],\n",
       " ['[CLS]', 'the', 'boy', 'kicks', 'a', 'round', 'object', '[SEP]'],\n",
       " ['[CLS]', 'the', 'male', 'child', 'kicks', 'the', 'ball', '[SEP]'],\n",
       " ['[CLS]', 'the', 'boy', 'is', 'playing', 'football', '[SEP]'],\n",
       " ['[CLS]', 'the', 'boy', 'hits', 'the', 'ball', '[SEP]'],\n",
       " ['[CLS]', 'the', 'ball', 'hits', 'the', 'boy', '[SEP]'],\n",
       " ['[CLS]', 'the', 'boy', 'is', 'hit', 'by', 'the', 'ball', '[SEP]'],\n",
       " ['[CLS]', 'the', 'ball', 'is', 'hit', 'by', 'the', 'boy', '[SEP]'],\n",
       " ['[CLS]', 'the', 'female', 'child', 'kicks', 'the', 'ball', '[SEP]'],\n",
       " ['[CLS]', 'the', 'girl', 'kicks', 'the', 'ball', '[SEP]'],\n",
       " ['[CLS]', 'the', 'child', 'plays', 'with', 'dolls', '[SEP]'],\n",
       " ['[CLS]', 'the', 'female', 'child', 'plays', 'with', 'dolls', '[SEP]'],\n",
       " ['[CLS]', 'the', 'male', 'child', 'plays', 'with', 'dolls', '[SEP]'],\n",
       " ['[CLS]', 'the', 'girl', 'plays', 'with', 'dolls', '[SEP]'],\n",
       " ['[CLS]', 'the', 'boy', 'plays', 'with', 'dolls', '[SEP]'],\n",
       " ['[CLS]', 'the', 'boy', 'is', 'kicking', 'the', 'ball', '[SEP]'],\n",
       " ['[CLS]', 'the', 'boy', 'is', 'not', 'kicking', 'the', 'ball', '[SEP]'],\n",
       " ['[CLS]', 'all', 'boys', 'kick', 'balls', '[SEP]'],\n",
       " ['[CLS]', 'every', 'boy', 'kicks', 'a', 'ball', '[SEP]'],\n",
       " ['[CLS]', 'there', 'is', 'a', 'boy', 'kicking', 'a', 'ball', '[SEP]'],\n",
       " ['[CLS]', 'there', 'is', 'not', 'a', 'boy', 'kicking', 'a', 'ball', '[SEP]'],\n",
       " ['[CLS]', 'i', 'can', 'see', 'a', 'boy', 'kicking', 'a', 'ball', '[SEP]'],\n",
       " ['[CLS]', 'i', 'cannot', 'see', 'a', 'boy', 'kicking', 'a', 'ball', '[SEP]'],\n",
       " ['[CLS]',\n",
       "  'i',\n",
       "  'believe',\n",
       "  'the',\n",
       "  'boy',\n",
       "  'is',\n",
       "  'kicking',\n",
       "  'the',\n",
       "  'ball',\n",
       "  '[SEP]'],\n",
       " ['[CLS]', 'no', 'boys', 'kick', 'balls', '[SEP]'],\n",
       " ['[CLS]', 'boys', 'do', 'not', 'kick', 'balls', '[SEP]'],\n",
       " ['[CLS]', 'boys', 'always', 'kick', 'balls', '[SEP]'],\n",
       " ['[CLS]',\n",
       "  'i',\n",
       "  'can',\n",
       "  'see',\n",
       "  'a',\n",
       "  'male',\n",
       "  'child',\n",
       "  'kicking',\n",
       "  'a',\n",
       "  'ball',\n",
       "  '[SEP]'],\n",
       " ['[CLS]', 'i', 'can', 'see', 'a', 'girl', 'kicking', 'a', 'ball', '.']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "nervous-effort",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    # See the models docstrings for the detail of the inputs\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors,output_hidden_states=True)\n",
    "   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eligible-juvenile",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.6674,  0.9458, -0.5036,  ..., -0.5302,  0.1874,  0.6336],\n",
       "          [ 0.3067, -0.0335,  0.0011,  ...,  0.1267, -0.3640, -0.9253],\n",
       "          [ 0.4349,  0.3408,  0.5464,  ..., -0.4371, -0.1396,  0.6499],\n",
       "          ...,\n",
       "          [ 0.2870,  0.5662, -0.0956,  ..., -0.5547, -0.4900, -0.2980],\n",
       "          [ 0.6253,  0.0615, -0.2790,  ...,  0.0038, -0.5781, -0.4948],\n",
       "          [ 0.6399,  0.0483, -0.2593,  ...,  0.0093, -0.5864, -0.4718]]]),\n",
       " tensor([[-0.9920, -0.8534, -0.9981,  0.9898,  0.9694, -0.7778,  0.9918,  0.7435,\n",
       "          -0.9951, -1.0000, -0.9537,  0.9990,  0.9948,  0.9125,  0.9905, -0.9797,\n",
       "          -0.9501, -0.9055,  0.7429, -0.9396,  0.9539,  1.0000, -0.7118,  0.8053,\n",
       "           0.8925,  1.0000, -0.9784,  0.9881,  0.9908,  0.8765, -0.9697,  0.7592,\n",
       "          -0.9981, -0.6847, -0.9970, -0.9994,  0.9093, -0.9470, -0.6564, -0.6737,\n",
       "          -0.9759,  0.8515,  1.0000,  0.4217,  0.8609, -0.7738, -1.0000,  0.7504,\n",
       "          -0.9733,  0.9991,  0.9951,  0.9958,  0.8245,  0.9274,  0.9106, -0.8983,\n",
       "           0.6171,  0.6578, -0.7522, -0.9329, -0.8695,  0.8373, -0.9920, -0.9770,\n",
       "           0.9985,  0.9901, -0.7740, -0.8020, -0.7395,  0.4467,  0.9907,  0.7470,\n",
       "          -0.6924, -0.9460,  0.9862,  0.7678, -0.8415,  1.0000, -0.9650, -0.9968,\n",
       "           0.9861,  0.9893,  0.8187, -0.9405,  0.9565, -1.0000,  0.9414, -0.6060,\n",
       "          -0.9974,  0.7915,  0.9118, -0.7666,  0.9441,  0.8653, -0.8785, -0.9049,\n",
       "          -0.8647, -0.9939, -0.7791, -0.8847,  0.6838, -0.7547, -0.9171, -0.7626,\n",
       "           0.7974, -0.9089, -0.9127,  0.9330,  0.8373,  0.9413,  0.7347, -0.7677,\n",
       "           0.9071, -0.9925,  0.9362, -0.7880, -0.9977, -0.8618, -0.9973,  0.8958,\n",
       "          -0.8342, -0.7956,  0.9952, -0.7976,  0.8478, -0.7864, -0.9984, -1.0000,\n",
       "          -0.9598, -0.9045, -0.7693, -0.8251, -0.9955, -0.9902,  0.9385,  0.9900,\n",
       "           0.7818,  1.0000, -0.8784,  0.9820, -0.8444, -0.9638,  0.9604, -0.8807,\n",
       "           0.9772,  0.8863, -0.9577,  0.7125, -0.9123,  0.8488, -0.9688, -0.8415,\n",
       "          -0.9844, -0.9808, -0.7662,  0.9912, -0.8856, -0.9991, -0.7581, -0.7853,\n",
       "          -0.8826,  0.9685,  0.9713,  0.8696, -0.9014,  0.8728,  0.8884,  0.9016,\n",
       "          -0.9784, -0.7636,  0.8841, -0.8225, -0.9879, -0.9964, -0.8620,  0.8534,\n",
       "           0.9980,  0.9479,  0.7873,  0.9910, -0.7581,  0.9552, -0.9899,  0.9966,\n",
       "          -0.7570,  0.5980, -0.7977,  0.8375, -0.9794,  0.5972,  0.9890, -0.9660,\n",
       "          -0.9679, -0.7693, -0.8489, -0.9049, -0.9854,  0.8821, -0.7518, -0.7079,\n",
       "          -0.6682,  0.9754,  0.9996,  0.9721,  0.9171,  0.9500, -0.9908, -0.8700,\n",
       "           0.7582,  0.7664,  0.7310,  0.9992, -0.9316, -0.7278, -0.9734, -0.9959,\n",
       "           0.6740, -0.9737, -0.7451, -0.9224,  0.9654, -0.8308,  0.9702,  0.8731,\n",
       "          -0.9997, -0.9499,  0.8576, -0.8373,  0.8823, -0.7450,  0.6808,  0.9977,\n",
       "          -0.9323,  0.9893,  0.9728, -0.9950, -0.9247,  0.9822, -0.8109,  0.9878,\n",
       "          -0.9496,  0.9998,  0.9973,  0.9920, -0.9888, -0.9745, -0.9904, -0.9865,\n",
       "          -0.7125,  0.8473,  0.9979,  0.8981,  0.9016, -0.5928, -0.9488,  1.0000,\n",
       "          -0.3508, -0.9907, -0.1536, -0.8569, -0.9974,  0.9897,  0.7493,  0.9048,\n",
       "          -0.8930, -0.9627, -0.9920,  0.9959,  0.7362,  0.9997, -0.8934, -0.9988,\n",
       "          -0.9553, -0.9907,  0.6157, -0.7970, -0.9377,  0.4954, -0.9907,  0.8796,\n",
       "           0.8464,  0.9228, -0.9960,  1.0000,  1.0000,  0.9947,  0.9719,  0.9923,\n",
       "          -1.0000, -0.6040,  1.0000, -1.0000, -1.0000, -0.9882, -0.9459,  0.7377,\n",
       "          -1.0000, -0.6635, -0.6586, -0.9673,  0.9837,  0.9952,  1.0000, -1.0000,\n",
       "           0.9634,  0.9909, -0.8944,  0.9994, -0.9058,  0.9945,  0.8854,  0.8465,\n",
       "          -0.7271,  0.8456, -0.9985, -0.9951, -0.9732, -0.9846,  1.0000,  0.6920,\n",
       "          -0.9485, -0.9853,  0.8946, -0.7972,  0.6410, -0.9914, -0.7770,  0.9767,\n",
       "           0.9624,  0.7348,  0.7770, -0.9365,  0.8198,  0.6665,  0.8833,  0.8675,\n",
       "          -0.9828, -0.8991, -0.8155,  0.6827, -0.9729, -0.9920,  0.9961, -0.8314,\n",
       "           0.9971,  1.0000,  0.8267, -0.9900,  0.9614,  0.8303, -0.5444,  1.0000,\n",
       "           0.9586, -0.9967, -0.8382,  0.9182, -0.9399, -0.9588,  1.0000, -0.7798,\n",
       "          -0.9867, -0.9522,  0.9948, -0.9978,  1.0000, -0.9881, -0.9905,  0.9944,\n",
       "           0.9871, -0.9434, -0.8677,  0.7637, -0.9775,  0.7947, -0.9945,  0.9808,\n",
       "           0.9346, -0.6588,  0.9742, -0.9940, -0.7931,  0.7875, -0.9799, -0.7599,\n",
       "           0.9980,  0.9043, -0.7754,  0.6658, -0.7986, -0.3307, -0.9923,  0.9022,\n",
       "           1.0000, -0.8227,  0.9825, -0.9080, -0.6014,  0.6226,  0.9159,  0.9415,\n",
       "          -0.7960, -0.9544,  0.9823, -0.9982, -0.9964,  0.9476,  0.7602, -0.8235,\n",
       "           1.0000,  0.9153,  0.7399,  0.8395,  1.0000,  0.6071,  0.9156,  0.9986,\n",
       "           0.9961, -0.7822,  0.8492,  0.9883, -0.9988, -0.8254, -0.9167,  0.6585,\n",
       "          -0.9827, -0.6974, -0.9918,  0.9918,  0.9995,  0.8936,  0.7949,  0.9690,\n",
       "           1.0000, -0.4737,  0.9129, -0.9544,  0.9918, -1.0000, -0.9814, -0.8310,\n",
       "          -0.7560, -0.9961, -0.7968,  0.7888, -0.9939,  0.9953,  0.9693, -0.9999,\n",
       "          -0.9981, -0.8581,  0.9874,  0.6808, -0.9999, -0.9777, -0.8569,  0.9685,\n",
       "          -0.8493, -0.9911, -0.9130, -0.8793,  0.8907, -0.7382,  0.7991,  0.9954,\n",
       "          -0.3901, -0.9847, -0.7989, -0.6627, -0.9784,  0.9864, -0.9725, -0.9983,\n",
       "          -0.7337,  1.0000, -0.7928,  0.9937,  0.9591,  0.9633, -0.7954,  0.6570,\n",
       "           0.9967,  0.8362, -0.9908, -0.9979, -0.9884, -0.8979,  0.9179,  0.9442,\n",
       "           0.9760,  0.9389,  0.9495,  0.7606, -0.6604,  0.7730,  1.0000, -0.7958,\n",
       "          -0.8235, -0.8818, -0.6877, -0.8061, -0.9523,  1.0000,  0.7959,  0.9194,\n",
       "          -0.9978, -0.9934, -0.9967,  1.0000,  0.9537, -0.9557,  0.9569,  0.9294,\n",
       "          -0.6975,  0.9865, -0.7497, -0.7547,  0.7697,  0.6506,  0.9939, -0.8970,\n",
       "          -0.9937, -0.8969,  0.8814, -0.9936,  1.0000, -0.9344, -0.6993, -0.7738,\n",
       "          -0.9229,  0.9756,  0.5358, -0.9948, -0.8095,  0.7254,  0.9943,  0.8051,\n",
       "          -0.8519, -0.9900,  0.9966,  0.9833, -0.9981, -0.9795,  0.9948, -0.9960,\n",
       "           0.9296,  1.0000,  0.7765,  0.9189,  0.8173, -0.8967,  0.8645, -0.8983,\n",
       "           0.9594, -0.9923, -0.8647, -0.7741,  0.8819, -0.8473, -0.8005,  0.9022,\n",
       "           0.7010, -0.7941, -0.9262, -0.8005,  0.8966,  0.9892, -0.7817, -0.7274,\n",
       "           0.7499, -0.7966, -0.9950, -0.8403, -0.9028, -1.0000,  0.9396, -1.0000,\n",
       "           0.9699,  0.8453, -0.6894,  0.9600,  0.5040,  0.9370, -0.9575, -0.9969,\n",
       "          -0.7215,  0.9566, -0.8462, -0.9478, -0.9579,  0.8933, -0.7940,  0.6971,\n",
       "          -0.9605,  0.9234, -0.8134,  1.0000,  0.7787, -0.9636, -0.9995,  0.7418,\n",
       "          -0.8454,  1.0000, -0.9968, -0.9925,  0.8237, -0.9792, -0.9584,  0.7873,\n",
       "           0.6013, -0.9759, -0.9990,  0.9966,  0.9955, -0.7298,  0.9346, -0.8549,\n",
       "          -0.9028,  0.5879,  0.9959,  0.9975,  0.9175,  0.9933, -0.5805, -0.7641,\n",
       "           0.9928,  0.8033,  0.9636,  0.6838,  1.0000,  0.8710, -0.9845, -0.5421,\n",
       "          -0.9976, -0.8036, -0.9960,  0.8349,  0.8067,  0.9849, -0.8317,  0.9972,\n",
       "          -0.9945,  0.6559, -0.9616, -0.9538,  0.8082, -0.9879, -0.9956, -0.9971,\n",
       "           0.9609, -0.8136, -0.6031,  0.7689,  0.7609,  0.9146,  0.8965, -1.0000,\n",
       "           0.9870,  0.9008,  0.9970,  0.9928,  0.9553,  0.9059,  0.7928, -0.9978,\n",
       "          -0.9997, -0.8499, -0.7567,  0.9731,  0.9451,  0.9759,  0.8792, -0.7774,\n",
       "          -0.8035, -0.9736, -0.6400, -0.9987,  0.8177, -0.9809, -0.9993,  0.9907,\n",
       "           0.5525, -0.6939, -0.7384, -0.9846,  0.9975,  0.9612,  0.8804,  0.6486,\n",
       "           0.8515,  0.9766,  0.9948,  0.9970, -0.9918,  0.9730, -0.9867,  0.8941,\n",
       "           0.5481, -0.9805,  0.7424,  0.9325, -0.8293,  0.7965, -0.7773, -0.9986,\n",
       "           0.5689, -0.7585,  0.9433, -0.8013, -0.6182, -0.8715, -0.6922, -0.9015,\n",
       "          -0.9589,  0.8264,  0.9391,  0.9861,  0.9824, -0.6870, -0.9766, -0.7581,\n",
       "          -0.9901, -0.9811,  0.9974, -0.7849, -0.9224,  0.9792,  0.6575,  0.7181,\n",
       "           0.7718, -0.8658, -0.8692, -0.9748,  0.9783, -0.7775, -0.9432, -0.9298,\n",
       "           0.9382,  0.8087,  1.0000, -0.9894, -0.9983, -0.8440, -0.8447,  0.7453,\n",
       "          -0.9005, -1.0000,  0.8348, -0.9474,  0.9618, -0.9782,  0.9927, -0.9642,\n",
       "          -0.9994, -0.8306,  0.8780,  0.9822, -0.8960, -0.9683,  0.8213, -0.9105,\n",
       "           0.9998,  0.9668, -0.9696, -0.6894,  0.8576, -0.9900, -0.9290,  0.9831]]),\n",
       " (tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
       "           [ 0.9868,  0.4540, -1.0180,  ...,  0.7542,  0.2929, -1.5443],\n",
       "           [-0.2330,  0.1164,  0.5087,  ...,  0.3019,  0.1804,  0.3744],\n",
       "           ...,\n",
       "           [ 0.9994,  0.9047, -1.2890,  ...,  0.8085, -0.8090, -0.2074],\n",
       "           [-0.1864,  0.2774, -0.2251,  ...,  0.7510,  0.4162,  0.3754],\n",
       "           [-0.4083, -0.0742, -0.2198,  ..., -0.0169,  0.0754, -0.1792]]]),\n",
       "  tensor([[[ 0.1878, -0.0751, -0.0687,  ..., -0.0758,  0.0329,  0.1157],\n",
       "           [ 1.0104,  0.7452, -1.0832,  ...,  0.4940,  0.1567, -1.8934],\n",
       "           [ 0.0868,  0.2392,  0.8932,  ..., -0.4396,  0.2071,  0.1481],\n",
       "           ...,\n",
       "           [ 1.5032,  0.8035, -0.8785,  ...,  0.0416, -0.6364, -0.5124],\n",
       "           [-0.1756,  0.2548, -0.0932,  ...,  0.3335,  0.2138,  0.2482],\n",
       "           [-0.1456,  0.0736, -0.0938,  ..., -0.0030,  0.4308,  0.0211]]]),\n",
       "  tensor([[[ 0.1896, -0.0662, -0.1031,  ..., -0.1848, -0.0491,  0.0516],\n",
       "           [ 0.8948,  1.1034, -0.5450,  ...,  0.5724,  0.1053, -2.0584],\n",
       "           [-0.3607,  0.4527,  1.2999,  ..., -0.7480,  0.0715, -0.0329],\n",
       "           ...,\n",
       "           [ 1.9348,  1.1485, -0.7927,  ...,  0.2501, -1.1114, -0.4855],\n",
       "           [-0.1928,  0.1680,  0.1940,  ..., -0.0887,  0.0998,  0.3689],\n",
       "           [-0.2485,  0.0031,  0.0798,  ..., -0.0797,  0.4023, -0.0918]]]),\n",
       "  tensor([[[-0.0099, -0.0595, -0.0674,  ..., -0.1763,  0.0344,  0.1454],\n",
       "           [ 0.6131,  0.5871, -0.0202,  ...,  0.8430,  0.1292, -1.9087],\n",
       "           [-0.9492,  0.1793,  0.3505,  ..., -0.5398, -0.1869, -0.0091],\n",
       "           ...,\n",
       "           [ 1.9983,  1.0839, -0.6500,  ..., -0.0065, -0.6949, -0.5403],\n",
       "           [-0.3257,  0.1625,  0.3971,  ..., -0.1528,  0.0231,  0.0504],\n",
       "           [-0.0940, -0.0621,  0.0881,  ...,  0.0595,  0.0472, -0.0133]]]),\n",
       "  tensor([[[ 2.4220e-02, -1.3628e-01, -2.8082e-01,  ..., -3.3888e-01,\n",
       "             1.3546e-01,  5.8827e-01],\n",
       "           [ 3.8286e-01, -5.5272e-03,  4.1677e-01,  ...,  9.9048e-01,\n",
       "            -3.8671e-01, -2.1947e+00],\n",
       "           [-9.7198e-01, -1.8916e-01,  6.5491e-01,  ..., -5.1022e-01,\n",
       "            -6.7464e-01,  8.0498e-02],\n",
       "           ...,\n",
       "           [ 2.1391e+00,  1.1571e+00, -3.3453e-01,  ..., -3.0247e-02,\n",
       "            -3.8332e-01, -4.6245e-01],\n",
       "           [-2.2154e-01,  3.9203e-01,  4.7023e-01,  ..., -5.4786e-01,\n",
       "             1.7319e-01,  1.7003e-01],\n",
       "           [-4.9989e-02, -2.6487e-02, -7.3699e-04,  ...,  2.8987e-02,\n",
       "             2.5606e-02, -1.0847e-02]]]),\n",
       "  tensor([[[ 2.2291e-02,  4.2560e-01, -3.5848e-01,  ..., -3.8712e-01,\n",
       "             2.7991e-01,  8.1028e-01],\n",
       "           [ 8.7145e-01, -4.7755e-01,  1.8456e-01,  ...,  5.6867e-01,\n",
       "            -4.4134e-01, -2.1077e+00],\n",
       "           [-5.8751e-01, -3.5007e-01,  2.2960e-01,  ..., -9.3782e-01,\n",
       "             8.0304e-02,  3.6373e-01],\n",
       "           ...,\n",
       "           [ 2.1366e+00,  9.0747e-01, -4.3225e-02,  ...,  3.7353e-01,\n",
       "            -7.3294e-02, -2.0660e-01],\n",
       "           [ 7.7743e-02,  2.9835e-01,  4.3609e-01,  ..., -1.7222e-01,\n",
       "             1.8098e-01,  6.6856e-02],\n",
       "           [-3.1906e-02, -2.6378e-02,  1.8792e-03,  ...,  2.5791e-02,\n",
       "            -2.1704e-02, -2.6086e-02]]]),\n",
       "  tensor([[[-2.5147e-01,  9.6250e-01, -9.3098e-01,  ..., -6.7206e-01,\n",
       "            -9.9479e-02,  8.7910e-01],\n",
       "           [ 8.0019e-01, -3.5505e-01, -5.1569e-02,  ...,  4.1974e-01,\n",
       "            -4.6207e-01, -2.0910e+00],\n",
       "           [-1.4063e-01,  1.5655e-01, -2.7989e-01,  ..., -1.3049e+00,\n",
       "             5.7566e-01,  1.6716e-01],\n",
       "           ...,\n",
       "           [ 2.2153e+00,  6.8225e-01, -2.2407e-01,  ..., -2.9005e-03,\n",
       "            -1.5447e-01,  1.8657e-03],\n",
       "           [ 1.9071e-01,  6.3617e-01,  1.1504e-01,  ..., -6.3296e-01,\n",
       "             4.4848e-02,  3.3372e-01],\n",
       "           [ 1.4959e-02,  3.8074e-03, -2.4339e-02,  ...,  2.0348e-02,\n",
       "            -2.4315e-02, -3.8528e-02]]]),\n",
       "  tensor([[[ 0.1075,  1.0070, -1.1332,  ..., -0.4228, -0.2015,  0.9617],\n",
       "           [ 1.1078,  0.0123, -0.4682,  ...,  0.1286, -0.3706, -2.1118],\n",
       "           [-0.0890,  0.0114, -0.1082,  ..., -1.4991,  0.8509,  0.0510],\n",
       "           ...,\n",
       "           [ 1.7290,  0.1479,  0.1721,  ..., -0.8372, -0.6202,  0.2057],\n",
       "           [ 0.1875,  0.2427,  0.4634,  ..., -0.4594,  0.6873,  0.1799],\n",
       "           [ 0.0435,  0.0140, -0.0314,  ..., -0.0033, -0.0108, -0.0581]]]),\n",
       "  tensor([[[-0.2115,  0.8808, -1.0971,  ..., -0.5508, -0.0234,  0.9936],\n",
       "           [ 0.6476, -0.1967, -0.7240,  ...,  0.2488, -0.5307, -1.5184],\n",
       "           [-0.1062, -0.0633, -0.5265,  ..., -1.3062,  0.3024,  0.6285],\n",
       "           ...,\n",
       "           [ 1.4874, -0.1015,  0.1410,  ..., -0.6806, -0.5149,  0.1774],\n",
       "           [ 0.1932,  0.1688, -0.0382,  ..., -0.3819,  0.3284,  0.1854],\n",
       "           [ 0.0585,  0.0456,  0.0231,  ..., -0.0249, -0.0634, -0.0358]]]),\n",
       "  tensor([[[-0.3265,  0.7796, -0.8486,  ..., -0.2415, -0.0544,  1.2388],\n",
       "           [ 0.8846, -0.0268, -0.3214,  ...,  0.5336, -0.7036, -1.6202],\n",
       "           [ 0.1849, -0.0824, -0.7020,  ..., -0.8908, -0.1001,  0.7738],\n",
       "           ...,\n",
       "           [ 1.4154,  0.1376,  0.1987,  ..., -0.6051, -0.3726,  0.3278],\n",
       "           [ 0.1159,  0.3690, -0.2104,  ..., -0.2336, -0.1170,  0.1151],\n",
       "           [ 0.0430,  0.0962, -0.0180,  ..., -0.0372, -0.0545,  0.0391]]]),\n",
       "  tensor([[[-0.0130,  0.6184, -0.3917,  ..., -0.3721, -0.2895,  1.3896],\n",
       "           [ 0.5188,  0.1178, -0.3327,  ...,  0.5222, -0.3539, -1.9068],\n",
       "           [ 0.0906,  0.0401, -0.6387,  ..., -0.7916, -0.2043,  1.1811],\n",
       "           ...,\n",
       "           [ 0.7623,  0.1608,  0.3415,  ..., -0.2797, -0.3516,  0.0500],\n",
       "           [ 0.0135,  0.0201, -0.0485,  ...,  0.0150, -0.0273,  0.0070],\n",
       "           [ 0.0166,  0.0117, -0.0412,  ...,  0.0443, -0.0142,  0.0160]]]),\n",
       "  tensor([[[-3.4760e-01,  6.1722e-01, -6.2986e-01,  ..., -5.5267e-02,\n",
       "            -4.7414e-01,  1.1962e+00],\n",
       "           [ 2.4667e-01, -1.1385e-01, -3.2224e-01,  ...,  3.9890e-01,\n",
       "            -4.0355e-01, -1.5206e+00],\n",
       "           [-1.5780e-01,  2.0670e-01, -3.2949e-01,  ..., -7.0685e-01,\n",
       "            -8.9300e-02,  1.3601e+00],\n",
       "           ...,\n",
       "           [ 3.1460e-01,  4.2031e-01,  1.2169e-01,  ..., -4.4048e-01,\n",
       "            -5.6069e-01, -1.8098e-01],\n",
       "           [ 4.6541e-02,  1.3745e-02, -3.7530e-02,  ...,  1.9264e-02,\n",
       "            -1.4171e-02, -4.5845e-03],\n",
       "           [ 4.5347e-02,  8.7522e-03, -3.7831e-02,  ...,  1.8386e-02,\n",
       "            -1.5910e-02, -1.1673e-03]]]),\n",
       "  tensor([[[-0.6674,  0.9458, -0.5036,  ..., -0.5302,  0.1874,  0.6336],\n",
       "           [ 0.3067, -0.0335,  0.0011,  ...,  0.1267, -0.3640, -0.9253],\n",
       "           [ 0.4349,  0.3408,  0.5464,  ..., -0.4371, -0.1396,  0.6499],\n",
       "           ...,\n",
       "           [ 0.2870,  0.5662, -0.0956,  ..., -0.5547, -0.4900, -0.2980],\n",
       "           [ 0.6253,  0.0615, -0.2790,  ...,  0.0038, -0.5781, -0.4948],\n",
       "           [ 0.6399,  0.0483, -0.2593,  ...,  0.0093, -0.5864, -0.4718]]])))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.to_tuple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "executed-protocol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "torch.Size([1, 25, 768])\n",
      "torch.Size([1, 768])\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(outputs))\n",
    "for i in range(len(outputs)):\n",
    "    try:\n",
    "        print(outputs[i].shape)\n",
    "    except:\n",
    "        print(len(outputs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-wright",
   "metadata": {},
   "source": [
    "Here:\n",
    "* outputs[0] contains the output representation of each token\n",
    "* outputs[1] is representation of the first token (after being put through an additional layer)\n",
    "* outputs[2] is a a tuple.  Each element is the hidden layer at depth n.  If we want the last layer then we need outputs[2][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "royal-kazakhstan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6674,  0.9458, -0.5036,  ..., -0.5302,  0.1874,  0.6336],\n",
       "         [ 0.3067, -0.0335,  0.0011,  ...,  0.1267, -0.3640, -0.9253],\n",
       "         [ 0.4349,  0.3408,  0.5464,  ..., -0.4371, -0.1396,  0.6499],\n",
       "         ...,\n",
       "         [ 0.2870,  0.5662, -0.0956,  ..., -0.5547, -0.4900, -0.2980],\n",
       "         [ 0.6253,  0.0615, -0.2790,  ...,  0.0038, -0.5781, -0.4948],\n",
       "         [ 0.6399,  0.0483, -0.2593,  ...,  0.0093, -0.5864, -0.4718]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#outputs[2][-1] is the last hidden layer also output as outputs[0]\n",
    "outputs[2][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "tutorial-category",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.4760e-01,  6.1722e-01, -6.2986e-01,  ..., -5.5267e-02,\n",
       "          -4.7414e-01,  1.1962e+00],\n",
       "         [ 2.4667e-01, -1.1385e-01, -3.2224e-01,  ...,  3.9890e-01,\n",
       "          -4.0355e-01, -1.5206e+00],\n",
       "         [-1.5780e-01,  2.0670e-01, -3.2949e-01,  ..., -7.0685e-01,\n",
       "          -8.9300e-02,  1.3601e+00],\n",
       "         ...,\n",
       "         [ 3.1460e-01,  4.2031e-01,  1.2169e-01,  ..., -4.4048e-01,\n",
       "          -5.6069e-01, -1.8098e-01],\n",
       "         [ 4.6541e-02,  1.3745e-02, -3.7530e-02,  ...,  1.9264e-02,\n",
       "          -1.4171e-02, -4.5845e-03],\n",
       "         [ 4.5347e-02,  8.7522e-03, -3.7831e-02,  ...,  1.8386e-02,\n",
       "          -1.5910e-02, -1.1673e-03]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so if you want the penultimate hidden layer you need outputs[2][-2]\n",
    "outputs[2][-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-humor",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "* Encode each sentence using the output representation for its CLS token - note that you do not need to mask the CLS token.  We are just interested in the output layer embedding for this token.  You can use outputs[0][0] or outputs[1] as a representation of the CLS token - but you will get different results as outputs[1] as gone through an additional layer (trained for next sentence prediction during fine-tuning and classification IF the model has been fine-tuned).\n",
    "* Use cosine similarity to determine all pairs similarities for the sentences.\n",
    "* Identify the 10 most similar pairs of sentences using this sentence encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "unusual-transsexual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-6.6740e-01,  9.4580e-01, -5.0361e-01,  7.6330e-02, -1.2090e+00,\n",
      "         1.2795e-01,  9.3474e-01,  1.1574e+00, -1.9790e-01,  9.3847e-02,\n",
      "        -7.1620e-01, -4.7296e-01, -2.1994e-01,  1.0530e+00,  3.7979e-01,\n",
      "        -1.6461e-01, -2.0667e-01,  1.0136e+00,  3.5675e-01, -2.8986e-02,\n",
      "         1.4404e-01, -2.0457e-01, -1.0593e-01,  9.7721e-03,  9.4197e-03,\n",
      "        -5.7983e-01, -1.4214e-01, -4.3860e-01,  6.5073e-01, -4.3919e-01,\n",
      "        -2.3003e-01,  1.1294e+00, -5.0243e-01, -1.7202e-01,  3.5664e-01,\n",
      "        -6.0078e-02,  1.5558e-01, -5.6304e-02,  3.3006e-01,  1.5681e-01,\n",
      "        -1.7278e-01,  1.3906e-01,  5.0966e-01,  3.3143e-01, -1.8095e-01,\n",
      "        -3.5913e-01, -1.8010e+00, -2.8397e-01, -3.3440e-01, -3.4394e-01,\n",
      "         1.7959e-01, -6.9788e-02,  8.8394e-01,  4.0846e-01, -3.3890e-01,\n",
      "         1.4026e+00, -9.4726e-01,  3.8425e-01,  1.7327e-01,  9.4517e-01,\n",
      "        -6.2579e-02, -7.5702e-02, -6.6680e-01, -4.6924e-01,  4.4900e-01,\n",
      "         9.2037e-01, -2.3056e-01,  8.5764e-01, -8.6860e-01,  4.7084e-01,\n",
      "        -1.1902e+00, -3.2860e-01,  1.0397e+00,  3.0817e-01, -4.8567e-01,\n",
      "         1.4067e-01, -2.4032e-02, -5.6979e-02, -1.6964e-01, -7.0868e-01,\n",
      "        -3.0032e-01,  1.1778e+00,  6.9872e-02,  1.9421e-01,  2.9240e-01,\n",
      "         4.3828e-01, -7.6978e-01, -6.2159e-01,  2.8800e-02,  9.7471e-01,\n",
      "        -5.9622e-01, -1.1416e-01,  9.1989e-02,  3.6834e-01,  1.5232e-01,\n",
      "        -4.7043e-01,  2.3515e-01, -2.9098e-01,  8.8956e-02,  1.0576e+00,\n",
      "         1.7939e-01,  1.2376e-01,  8.2968e-01, -5.0684e-01,  5.6706e-01,\n",
      "        -2.0957e-01, -5.6403e-01, -6.9694e-02,  6.7254e-01, -2.4246e+00,\n",
      "         3.7075e-01,  6.2552e-02, -1.3087e+00, -2.4063e-01, -1.6477e-01,\n",
      "         9.4754e-01,  6.8219e-01, -1.6098e-01,  3.2634e-01, -2.1106e-01,\n",
      "        -2.9645e-01,  6.7521e-02, -7.2849e-01, -1.1178e-01, -8.2746e-01,\n",
      "         6.5379e-01,  4.0921e-01, -2.2108e-01,  9.0913e-01,  4.3992e-01,\n",
      "        -1.8877e-01,  8.6647e-01,  2.8880e-01, -4.5078e-01, -1.0170e-01,\n",
      "        -4.3986e-01,  4.8777e-01,  6.0497e-01,  3.0857e-02,  4.7058e-02,\n",
      "        -3.4385e-01, -5.2550e-01, -2.4673e+00, -2.2917e-01,  6.9254e-01,\n",
      "         3.8068e-01, -3.6974e-01, -2.2660e-01,  6.1755e-01,  2.4143e-01,\n",
      "         2.0336e-01, -8.8882e-01,  4.0455e-01,  3.5941e-01, -3.4577e-01,\n",
      "        -4.0526e-01,  5.4672e-02, -5.1193e-01,  1.1819e+00,  3.8866e-01,\n",
      "        -1.9277e-01, -3.1344e-01,  2.3146e-01,  5.0904e-01, -8.8233e-01,\n",
      "         5.4669e-01,  9.3161e-01,  5.3567e-02, -2.2892e-01,  5.2604e-01,\n",
      "        -1.1699e+00,  2.1122e-02,  7.7599e-01, -3.2805e-01,  6.3798e-01,\n",
      "        -2.9925e-01,  4.5614e-01,  4.7945e-01,  3.1654e-01, -3.1910e-01,\n",
      "        -3.1567e-01,  5.7835e-01,  1.8409e-01,  3.1043e-01,  5.1138e-01,\n",
      "        -2.9181e-01,  7.0780e-01, -3.5996e-01, -1.2918e-01,  4.2341e-01,\n",
      "        -7.9498e-02, -1.3966e-01,  4.0272e-01,  1.9161e-01,  8.5323e-01,\n",
      "        -2.2554e-02,  5.6472e-01, -6.1660e-01,  3.4306e-01,  8.2930e-01,\n",
      "        -1.0811e-01, -1.5321e-01, -3.0524e-01,  4.3786e-01, -5.1105e-01,\n",
      "         3.3641e+00,  4.3857e-01,  1.6934e-01,  8.6702e-01,  2.9070e-01,\n",
      "         3.2152e-01,  3.0186e-01,  8.0267e-01, -3.6412e-01,  3.2114e-01,\n",
      "        -8.3257e-02, -8.9604e-02,  3.4259e-01, -5.1924e-01, -2.4413e-01,\n",
      "        -1.4795e-01,  1.6816e-01, -3.5884e-01,  5.3055e-01,  4.4307e-01,\n",
      "         4.1894e-01,  1.0622e-01,  2.4817e-01, -4.5394e-01, -7.2784e-01,\n",
      "         9.7641e-01, -7.7481e-02,  5.3338e-01,  4.0313e-01, -1.2049e+00,\n",
      "        -6.8491e-01, -6.3579e-01, -6.6552e-01, -4.1859e-01, -3.3715e-01,\n",
      "         3.0639e-01,  1.6492e-01, -5.7925e-01, -3.7338e-01, -3.1983e-01,\n",
      "         2.1585e-01,  3.6299e-01,  1.4850e-01,  2.6189e-01,  2.3879e-01,\n",
      "         2.1548e-01, -6.8556e-01,  6.2623e-01, -9.5585e-01, -5.1478e-02,\n",
      "        -2.0382e-01, -8.3097e-02,  1.8789e-01, -8.5968e-03, -1.2277e-01,\n",
      "        -1.8059e-01, -2.9426e-01,  5.5130e-01,  7.6095e-02, -1.0988e+00,\n",
      "        -1.1064e+00, -2.5757e-01,  7.6243e-02, -2.5939e-01, -3.3558e-01,\n",
      "         1.9520e-01, -4.7766e-01, -1.1611e-01, -2.8115e+00,  5.5441e-01,\n",
      "        -1.8944e-01, -1.5243e-01,  7.2636e-02, -7.4335e-01,  7.3357e-01,\n",
      "         5.5081e-01,  8.9753e-01, -6.3250e-01,  3.3272e-01, -9.9308e-01,\n",
      "         1.3288e-03,  6.2839e-01, -2.5147e-01,  2.5518e-01, -2.6105e-01,\n",
      "        -3.3124e-01, -5.8867e-01,  2.7250e-02,  2.0748e-01, -2.2294e-01,\n",
      "        -1.7389e-01, -1.5892e-01, -3.8122e-01, -2.0711e-01, -3.4738e-02,\n",
      "        -2.5499e-01, -1.7734e-01, -9.3978e-01,  1.7215e-01, -7.8436e-01,\n",
      "         1.0991e-01, -1.1592e-01,  9.9066e-02, -1.1170e+00,  4.3972e-01,\n",
      "         1.9943e-01, -3.1572e-01,  2.8662e-01, -8.7572e-02,  7.9462e-01,\n",
      "        -3.5997e-01, -1.2174e+00,  5.2946e-01, -5.2777e-01, -5.4497e-01,\n",
      "        -5.6919e-01,  7.4264e-01,  2.1747e-01,  2.8808e-01,  7.6460e-01,\n",
      "        -4.8651e-02,  1.2436e-01,  3.1120e-02, -4.7701e-01,  3.4804e-01,\n",
      "        -2.5628e-01, -2.4857e-01,  6.4914e-01,  4.9079e-01,  1.5116e-01,\n",
      "        -8.7774e-02, -3.9235e-01,  3.7593e-01, -3.2711e-01,  2.5793e-01,\n",
      "         1.8958e-01, -4.4051e-01, -8.9796e-01,  9.6463e-02,  1.4170e-01,\n",
      "         2.1279e-01,  3.9481e-01, -3.2454e-01, -7.8815e-01,  2.5108e-02,\n",
      "         9.3080e-02,  5.9011e-01,  7.7913e-01, -2.7237e-01, -4.7161e-01,\n",
      "        -1.2207e-01,  5.3882e-02,  2.3138e-01, -4.1126e-01,  1.3383e-01,\n",
      "         1.1416e+00,  1.5800e-01, -2.0349e-01, -3.0697e-01,  1.0403e+00,\n",
      "        -7.1395e-02,  2.9326e-01, -1.2865e-01,  1.1477e+00, -2.6029e-01,\n",
      "         4.6532e-01,  1.5326e-01,  8.1022e-01, -1.4058e+00, -1.5984e-01,\n",
      "        -4.0350e-01, -1.2381e-01,  6.3821e-01,  1.7226e-01,  7.5017e-01,\n",
      "        -6.9502e-01, -6.6469e-01, -2.6255e-02, -5.6596e-01, -3.1603e-01,\n",
      "         4.6462e-01,  6.0223e-01,  3.6813e-01, -2.0499e-01, -1.7435e-01,\n",
      "        -1.2028e+00,  5.9364e-01,  5.6916e-01,  8.5300e-02, -2.0547e-01,\n",
      "         2.5087e-01, -8.8546e-01, -6.8360e-01, -3.3005e-01,  5.4998e-01,\n",
      "         6.3073e-01,  6.9859e-01, -2.6504e-01,  4.2361e-02,  7.3518e-01,\n",
      "        -3.5852e-01,  6.2170e-01, -5.7687e-01, -2.9432e-01, -7.5390e-01,\n",
      "         2.0893e-01,  1.6238e-01, -2.9089e-01, -6.4752e-02, -1.2353e-01,\n",
      "         1.7404e-01, -1.6941e-01,  6.2851e-01, -1.8589e-01, -1.6391e-02,\n",
      "        -6.4829e-02,  1.8691e-01,  7.9685e-01,  3.1080e-01, -1.2678e-01,\n",
      "         8.4880e-01,  4.1906e-01,  8.2147e-01,  5.0436e-01,  3.8134e-01,\n",
      "         1.9335e-01, -8.5362e-01, -4.7178e-01, -1.0232e-01,  4.9575e-01,\n",
      "        -4.3223e-01, -4.1342e-01, -3.1547e-01, -3.0003e-01, -4.3377e-01,\n",
      "        -3.7583e-01, -6.3916e-01,  1.9551e-02, -2.4496e-01, -2.0752e-02,\n",
      "         1.1802e-01,  1.3051e-01, -3.5114e-01,  4.0449e-01,  1.1090e-01,\n",
      "        -5.5814e-01,  4.9496e-01,  4.0182e-02,  8.8175e-01,  3.3322e-01,\n",
      "        -1.9028e-01, -6.8003e-01,  2.1376e-02, -5.3601e-01, -5.9620e-01,\n",
      "        -1.7686e-01, -9.6230e-01,  1.5528e-01,  5.2032e-02,  4.5542e-02,\n",
      "         2.7541e-01, -2.4839e-02, -5.8453e-01, -1.3960e-01,  3.0781e-01,\n",
      "        -1.0028e+00, -1.2819e-01,  6.2772e-01, -4.0983e-01, -3.0887e-03,\n",
      "        -2.2511e-01, -7.6442e-01,  6.8231e-01, -3.9717e-01,  2.0949e-01,\n",
      "        -3.5221e-01, -5.3640e-01,  4.2392e-01, -1.2159e-01,  1.9132e-01,\n",
      "         3.6890e-01,  9.2834e-02, -6.1476e-01, -3.9579e-01, -2.7676e-02,\n",
      "        -3.1010e-01,  5.1340e-01,  7.8428e-02, -1.9585e-01, -2.5119e-01,\n",
      "        -3.7082e-01, -7.8127e-01,  4.6326e-01, -2.2708e-01,  6.2857e-01,\n",
      "        -9.4254e-02, -8.6835e-01, -9.8331e-01, -6.1424e-01, -7.0709e-02,\n",
      "        -1.8560e-02,  8.2780e-01, -1.0939e+00,  1.2963e+00,  4.6576e-01,\n",
      "        -6.6037e-02,  6.2975e-01,  6.0202e-01, -8.4167e-01,  1.2952e-01,\n",
      "         6.4952e-01, -4.4175e-01,  5.0394e-01,  1.3314e-01, -1.9608e-01,\n",
      "        -5.2174e-02,  8.4321e-02, -1.9377e-01, -5.8389e-01,  6.9108e-01,\n",
      "        -3.5703e-02, -2.2884e-01, -1.1306e-02, -5.4155e-01, -5.3725e-02,\n",
      "        -3.5426e-01, -5.1736e-01, -6.8069e-01,  4.4947e-01, -5.4171e-01,\n",
      "        -6.1554e-01,  7.9103e-01, -1.0076e+00, -7.1541e-01,  3.4459e-01,\n",
      "         2.6330e-01,  5.4404e-02, -4.1122e-01,  3.7553e-01, -5.5839e-01,\n",
      "         5.0389e-01, -1.3331e-02,  3.5283e-02,  5.5119e-02,  2.4099e-01,\n",
      "         5.3178e-01, -4.0029e-01, -4.5434e-01, -1.3725e-01,  4.5073e-01,\n",
      "         1.5820e-02, -6.4217e-01, -2.6076e-01, -2.0190e-01, -3.8685e-01,\n",
      "        -9.2641e-01, -8.9073e-01,  9.7078e-01, -4.9514e-01,  2.3998e-01,\n",
      "         1.1570e-02, -4.3794e-02, -3.6548e-01, -8.2815e-02,  2.5287e-01,\n",
      "        -2.2564e-01,  5.5260e-01,  2.7756e-01, -3.1936e-01,  1.2364e-01,\n",
      "         1.2992e+00,  8.2253e-01, -1.6216e-01, -2.8127e-01, -1.9145e-01,\n",
      "        -4.0202e-01, -1.7151e-01,  1.5738e-01, -1.2331e-01,  7.0661e-01,\n",
      "        -1.0176e+00, -1.1714e-01, -1.2592e+00,  1.3901e+00,  4.4673e-01,\n",
      "         4.7428e-01, -4.8312e-01,  8.7902e-01, -5.2070e-01, -9.9673e-02,\n",
      "        -1.1396e-01, -5.2742e-01,  3.7699e-01, -6.5867e-01, -3.8331e-01,\n",
      "        -3.1016e-01,  4.4299e-01,  7.5155e-01,  4.3820e-01, -5.3230e-01,\n",
      "        -8.0889e-02, -4.8584e-01,  1.9351e-02, -8.1617e-01,  1.1474e+00,\n",
      "         3.5502e-01,  3.4397e-01,  7.0018e-01,  3.0013e-02, -2.1908e-01,\n",
      "         1.1023e-01,  3.9017e-01,  6.8645e-01, -4.3876e-01,  4.7409e-01,\n",
      "        -1.1305e-01,  1.0065e+00,  1.7304e-01,  4.2045e-02,  2.2059e-01,\n",
      "        -9.0484e-01, -3.3388e-01,  1.0922e+00, -8.3941e-02, -9.9888e-01,\n",
      "         6.6390e-01, -1.6283e-01,  1.5321e-01,  7.4428e-01, -3.2533e-01,\n",
      "        -5.3569e-01,  8.1608e-02, -1.8912e-02,  3.4249e-01, -9.9655e-02,\n",
      "        -9.9712e-01,  1.8448e-01, -5.4033e-01, -1.7282e-01,  8.8372e-02,\n",
      "        -9.2202e-02, -1.5629e-01,  7.7086e-01, -4.3689e-02,  1.0868e+00,\n",
      "         2.2141e-01, -5.4605e-01, -6.8471e-01,  6.5064e-02, -2.3383e-01,\n",
      "         9.0462e-01,  1.1526e-01, -6.2660e-01, -6.6233e-03,  9.3954e-02,\n",
      "         5.5547e-01, -2.2430e-01,  3.7673e-01, -8.4830e-02,  5.5366e-01,\n",
      "        -9.6829e-02,  1.5077e-01, -2.3043e+00, -5.3875e-01, -3.5429e-01,\n",
      "         3.7661e-01,  3.3981e-01,  9.5326e-01, -3.2908e-01,  8.1280e-02,\n",
      "         1.1020e-01, -3.5380e-01,  5.7938e-02,  1.5098e-01,  5.4818e-01,\n",
      "         2.9483e-01,  2.5743e-02,  8.9497e-02,  5.5480e-01, -2.5026e-01,\n",
      "        -2.1588e-01,  4.8701e-02, -5.5741e-01, -2.7332e-01,  2.0498e-01,\n",
      "        -1.9226e-01, -7.1594e-01,  4.5581e-01,  5.2863e-02, -2.0981e-01,\n",
      "         2.8480e-01,  1.1746e+00, -2.3350e-01,  7.1755e-01,  2.3455e-01,\n",
      "         2.9920e-01, -9.6464e-02, -2.3288e-01, -4.7658e-01,  7.0057e-02,\n",
      "         2.7475e-01, -2.9672e-01, -2.8575e-01,  5.8955e-02,  1.2950e-01,\n",
      "        -5.3878e-02, -2.9274e-01, -5.2541e-01,  6.6574e-01, -2.3546e-01,\n",
      "         1.1864e+00, -3.0109e-01,  1.3985e-01, -1.7385e-01, -6.6286e-01,\n",
      "        -3.6984e-01,  3.8112e-01, -6.5707e-02, -2.4742e-01,  9.9321e-02,\n",
      "        -3.9066e-01, -2.0028e-01,  2.0686e-01,  2.1707e-01, -3.9271e-01,\n",
      "        -3.0403e-01,  1.8488e-02, -2.2376e-01, -6.5105e-01, -8.9502e-02,\n",
      "        -5.7498e-02, -5.3365e-02, -4.5792e-01,  7.2860e-02,  1.3909e+00,\n",
      "         2.9599e-02, -2.6510e-01,  5.4751e-02,  2.9000e-01,  2.5214e-01,\n",
      "         8.9416e-01,  7.3907e-02, -6.8553e-02,  4.2690e-01, -4.8072e-01,\n",
      "         5.4342e-01,  4.5997e-01, -5.8937e+00, -7.6322e-01, -6.5320e-01,\n",
      "        -5.9052e-01,  7.6064e-01, -9.5550e-02,  2.8271e-01, -3.8772e-01,\n",
      "        -3.4043e-01,  1.7632e-01, -6.3767e-01, -8.6609e-02, -9.0864e-02,\n",
      "        -5.3022e-01,  1.8736e-01,  6.3362e-01]) tensor([ 3.0667e-01, -3.3533e-02,  1.0749e-03,  4.4275e-01,  3.2029e-01,\n",
      "        -2.6730e-02, -1.5691e-01,  1.1625e+00, -2.0618e-01, -2.2093e-02,\n",
      "        -7.4281e-02, -4.9072e-01, -4.7213e-04,  4.0149e-01, -4.1905e-01,\n",
      "         1.0091e-01,  6.2423e-01, -1.7302e-01,  1.2183e-01, -2.8289e-01,\n",
      "         9.6517e-02,  6.5701e-01,  2.6484e-01, -3.2353e-01, -9.9295e-02,\n",
      "        -5.1268e-01, -9.4364e-02, -2.7154e-01,  4.8920e-01,  4.2688e-02,\n",
      "        -2.6435e-02,  4.5403e-01, -2.7992e-01, -5.4655e-01, -1.3765e-01,\n",
      "        -3.6692e-01,  4.4758e-01, -1.4239e-01, -4.1255e-01,  2.4425e-01,\n",
      "        -7.1896e-01, -2.0170e-01,  6.3411e-03,  1.2347e+00, -3.1898e-01,\n",
      "        -6.8181e-01,  3.8732e-01, -3.7707e-01, -1.2271e-01, -1.4587e-01,\n",
      "        -3.6772e-02,  7.8068e-01,  8.5411e-01,  4.9214e-01, -4.8294e-01,\n",
      "        -1.7819e-01, -6.3786e-01, -3.6930e-01, -3.2711e-01, -3.3182e-01,\n",
      "         2.2415e-01,  3.5899e-01, -3.2835e-02,  4.1382e-02,  1.2450e+00,\n",
      "         2.0317e-01,  8.3671e-01,  4.9295e-01, -5.1438e-01,  1.0706e-01,\n",
      "         5.6193e-01, -6.4334e-01, -6.1415e-01,  1.1681e-01,  1.1104e+00,\n",
      "        -3.7178e-01, -8.5009e-02,  8.1951e-02, -3.2775e-02, -3.7218e-01,\n",
      "        -6.4762e-01,  3.1243e-01, -8.9423e-01,  4.9363e-01, -7.6426e-01,\n",
      "        -3.7836e-01, -4.6097e-01,  7.8962e-01,  1.7101e-01,  7.6952e-01,\n",
      "         4.7635e-01, -3.6583e-01, -4.4496e-01,  3.0118e-01, -1.2286e-01,\n",
      "         2.8040e-02,  6.1499e-01,  2.4582e-01, -1.4291e-01,  1.2782e-01,\n",
      "        -4.5667e-01, -8.7483e-01,  3.9512e-01, -5.1047e-01, -4.5077e-01,\n",
      "        -1.3423e-01, -1.8840e-01,  3.8904e-01,  3.6842e-01,  1.5889e-01,\n",
      "         1.3033e-01,  1.7403e-01, -1.9520e-01,  5.1435e-02, -4.9269e-01,\n",
      "         4.8417e-01,  5.0251e-01, -4.3680e-01,  2.9263e-01,  4.3156e-01,\n",
      "        -7.1657e-02, -1.0872e-02,  4.0260e-01,  1.2633e-01,  6.5683e-01,\n",
      "         7.9791e-02, -6.1925e-01,  1.3173e-01, -6.4816e-01, -6.3237e-01,\n",
      "        -1.1164e-01,  1.1027e+00,  5.9867e-01,  4.3747e-01, -3.5963e-01,\n",
      "         6.2791e-01, -5.1964e-04, -2.5158e-01,  3.6624e-01,  3.4455e-01,\n",
      "         6.0196e-01,  3.2179e-01,  1.1164e-01, -6.0467e-01,  5.9987e-01,\n",
      "        -1.9058e-01,  6.6858e-02, -2.2976e-01, -5.5191e-01, -1.9517e-01,\n",
      "         2.2850e-01,  3.3771e-01,  2.2834e-01,  2.9871e-01,  8.1473e-01,\n",
      "         2.9031e-01, -2.9281e-01,  2.7147e-01, -2.7205e-01, -8.5180e-02,\n",
      "        -2.2813e-01, -4.3509e-01, -9.2137e-02,  1.8061e-01, -1.8865e-01,\n",
      "        -7.3241e-01,  6.6998e-01,  2.9762e-01, -4.2102e-01,  3.9378e-01,\n",
      "         8.1210e-01, -1.4014e-01,  2.7677e-01, -4.9307e-01, -6.0383e-01,\n",
      "        -3.6778e-01, -5.0654e-02, -2.3362e-01,  9.8354e-01, -8.3922e-02,\n",
      "        -3.3101e-01,  4.3763e-01,  1.3514e-01,  4.8411e-01,  4.1684e-01,\n",
      "        -2.0011e-01,  7.6374e-01, -5.9128e-02, -1.4640e-01,  3.0664e-01,\n",
      "        -3.1972e-03, -1.8512e-02,  3.8891e-01,  1.2498e-01,  1.1074e-01,\n",
      "        -7.1085e-01,  4.9549e-01, -3.0350e-01,  2.7973e-01, -1.5082e-01,\n",
      "         1.0918e-02,  8.7896e-01,  2.2012e-01,  1.6145e-01, -6.5695e-01,\n",
      "        -4.0485e-02,  3.9664e-01,  6.4327e-02,  1.2115e-01, -7.7427e-01,\n",
      "         5.4080e-01, -2.0258e-01, -3.6133e-01, -4.6346e-01,  1.7169e-01,\n",
      "        -4.6177e-01, -5.7242e-01, -3.4086e-01,  6.2519e-01,  3.3814e-01,\n",
      "         7.1200e-01, -9.9155e-01,  2.8915e-01,  3.1273e-01, -9.6509e-01,\n",
      "        -1.4588e-01,  2.1139e-01, -7.0425e-01,  5.9164e-01,  4.8681e-01,\n",
      "         9.7042e-02, -7.1456e-01,  9.2102e-01, -6.4809e-02,  4.6707e-01,\n",
      "        -8.7948e-02, -1.4600e-01, -5.2997e-01, -3.2284e-01, -2.4691e-01,\n",
      "        -5.6349e-02,  1.1274e+00,  1.4644e-01,  2.0490e-01,  9.6958e-02,\n",
      "        -7.2059e-01,  1.6085e-01,  6.8498e-02, -4.3314e-01,  2.8656e-02,\n",
      "        -7.4291e-01, -4.9677e-01,  3.4826e-02, -2.2077e-01,  5.7356e-02,\n",
      "        -3.4108e-01, -9.7976e-02, -4.3955e-01,  3.9767e-01,  5.7848e-01,\n",
      "         3.1014e-01, -3.3086e-01,  8.2937e-01, -2.5437e-01, -4.1137e-01,\n",
      "        -7.0100e-01,  2.6319e-01,  1.7358e-01, -8.2820e-02,  4.6205e-02,\n",
      "         1.0902e+00,  2.2257e-01,  5.1592e-01,  1.7767e-01, -7.6319e-02,\n",
      "        -1.4638e+00, -6.1887e-02, -1.1951e-02,  2.8668e-01,  2.1781e-01,\n",
      "         1.5340e-01,  9.1381e-01,  6.5248e-01,  6.1326e-02, -2.4523e-01,\n",
      "        -7.9657e-02,  9.4573e-01,  2.9124e-02, -7.2830e-02, -5.4457e-01,\n",
      "        -6.9149e-01,  1.5890e-01, -4.2303e-01, -7.7417e-01,  1.0518e+00,\n",
      "        -5.0467e-02,  1.5799e-01,  4.3011e-02,  8.1751e-03, -1.0243e+00,\n",
      "        -2.7836e-01, -9.1407e-02, -4.3162e-01,  5.7415e-02, -2.8216e-01,\n",
      "        -1.4208e-01, -7.0225e-01, -1.4403e-01, -4.9066e+00, -8.4784e-01,\n",
      "         9.9937e-01, -5.7613e-01, -1.6073e-01, -4.2822e-01,  3.3643e-01,\n",
      "        -4.2142e-01, -7.1507e-01,  4.6389e-01,  2.1143e-01, -4.2350e-02,\n",
      "         4.4469e-01,  8.8837e-01, -4.0169e-01, -5.4593e-01,  3.5594e-01,\n",
      "         2.4216e-01, -4.4335e-01,  2.9744e-01, -5.3396e-01, -2.8734e-01,\n",
      "         1.0109e-01, -5.0626e-01, -3.0190e-01,  3.0362e-01,  6.8892e-02,\n",
      "         1.6905e-01,  1.0720e-01, -4.1306e-02,  4.2618e-01,  3.0142e-01,\n",
      "         2.8206e-01,  5.1490e-01, -7.5329e-01,  1.6415e-01, -1.7633e-01,\n",
      "        -4.6045e-01,  8.1390e-02,  3.6582e-02, -2.3605e-01,  1.3011e-01,\n",
      "         3.2164e-01,  1.5381e-01,  1.0331e+00, -6.5726e-01,  4.1976e-01,\n",
      "        -4.5490e-01,  1.0347e-04,  9.6430e-01,  3.2615e-01, -4.1549e-01,\n",
      "         7.2616e-01,  6.3262e-01, -3.1601e-01,  2.5063e-01,  5.6600e-01,\n",
      "         1.0678e+00, -4.2936e-02, -6.4846e-01,  3.0794e-01, -8.7929e-01,\n",
      "        -4.6600e-01, -2.8208e-01, -4.9757e-01, -8.2367e-01, -5.7190e-01,\n",
      "        -4.4045e-01,  2.9727e-01,  7.3110e-01, -3.4597e-01, -1.9331e-01,\n",
      "        -6.3029e-01, -7.8174e-01, -2.5071e-01, -4.0026e-01,  6.0404e-01,\n",
      "         9.6245e-03,  1.1589e-01,  6.1283e-01, -6.6005e-01,  1.7596e-01,\n",
      "        -2.0269e-01, -2.3671e-01, -6.4653e-01, -6.4445e-01, -1.9199e-01,\n",
      "         1.9487e-01, -2.4814e-01, -1.5660e-01,  3.0066e-01, -5.6214e-01,\n",
      "        -3.5008e-01, -1.1914e-01,  1.6034e-01, -5.2772e-02,  1.2424e-01,\n",
      "        -3.8933e-01,  1.1477e-01,  1.0069e-01,  3.2502e-02, -7.7355e-02,\n",
      "         3.0097e-01,  7.9980e-03, -1.0629e-01, -3.6130e-01,  2.7655e-01,\n",
      "         3.4095e-01,  4.7538e-01, -5.5082e-01, -1.4483e-01, -4.6070e-01,\n",
      "         1.8647e-01,  1.2608e-01, -5.7631e-02, -3.9136e-01,  1.5034e-02,\n",
      "         5.4111e-01,  5.4270e-01,  1.3659e-01, -3.4714e-01,  6.2010e-01,\n",
      "        -9.6292e-01, -7.0716e-01,  8.5955e-01, -3.4040e-01, -1.3047e-01,\n",
      "         2.4774e-02, -5.2698e-01,  8.5743e-01, -2.5805e-01, -4.1002e-01,\n",
      "        -3.0510e-01, -1.9949e-01,  6.2789e-01,  3.6851e-01, -8.4755e-01,\n",
      "        -1.5122e-01,  4.5255e-02, -1.0161e-01, -1.4755e-01,  3.4131e-01,\n",
      "        -5.3072e-01,  2.5030e-01,  1.3450e-01,  1.0854e+00, -1.9741e-02,\n",
      "        -2.3741e-01, -2.7139e-01,  3.9445e-02, -4.6475e-02, -3.5039e-01,\n",
      "         5.5390e-03,  8.6230e-02, -5.4990e-01, -7.9379e-01,  5.1597e-01,\n",
      "        -4.0489e-01, -2.3193e-01, -9.8356e-01, -6.6232e-02, -1.3408e-02,\n",
      "        -4.1251e-01, -2.1532e-01, -6.7994e-01,  5.9044e-01, -3.1517e-01,\n",
      "        -9.4973e-01,  2.8993e-01,  1.1519e-01,  4.4014e-01, -1.0434e-01,\n",
      "         6.7943e-01, -4.5324e-02, -3.8497e-01,  7.3918e-02,  6.4195e-01,\n",
      "        -6.4655e-01, -1.3758e-01, -1.3325e-01,  1.0212e-01, -3.9202e-03,\n",
      "         5.7144e-01,  3.4736e-02, -5.2905e-02,  2.8033e-01,  5.5774e-01,\n",
      "         1.0824e-01, -5.4268e-01, -7.6452e-02,  8.2369e-01,  6.0344e-01,\n",
      "        -4.9621e-01, -4.1320e-01, -8.8904e-01,  4.9827e-01,  6.3094e-01,\n",
      "        -1.7789e-01,  4.3664e-01,  3.3260e-01, -8.3538e-01, -4.6461e-01,\n",
      "        -7.5608e-02,  4.6225e-02,  1.2492e-01, -8.7609e-01,  1.3545e-01,\n",
      "         3.6125e-01,  3.6187e-01,  1.6601e-01, -1.0225e+00,  5.3059e-02,\n",
      "        -5.7565e-01,  2.2998e-01, -1.4223e+00, -1.2301e-01, -3.3977e-01,\n",
      "         8.0948e-02, -1.2195e-01, -6.1156e-01, -2.8078e-03, -6.9777e-02,\n",
      "        -2.9610e-01, -6.3686e-01, -1.0589e+00, -4.8471e-02,  5.3173e-01,\n",
      "        -1.4490e-01,  4.0267e-01, -3.3775e-01,  9.5787e-01, -8.5017e-01,\n",
      "         4.0323e-01,  4.4814e-01,  2.3140e-02,  4.1542e-01,  8.2443e-01,\n",
      "        -5.3148e-01,  1.1808e-02,  3.9615e-01, -1.8924e-01, -4.4527e-02,\n",
      "         2.2675e-01, -9.4292e-02,  2.9077e-01, -7.5685e-01,  7.1830e-01,\n",
      "         8.3090e-02,  4.9717e-02, -2.4040e-01,  2.0570e-02,  6.7326e-01,\n",
      "         1.2027e-01, -4.9504e-01,  5.5387e-01,  3.0724e-01,  3.0695e-02,\n",
      "         2.3908e-01, -2.9771e-01, -5.0016e-02, -1.0509e-01,  9.4569e-02,\n",
      "         2.3732e-02,  1.0903e-01,  4.2101e-01,  1.0155e-01,  5.9126e-03,\n",
      "         4.9465e-01,  3.4485e-01,  3.9819e-01,  6.1088e-01,  3.8279e-01,\n",
      "         5.3023e-01, -3.9308e-01,  4.8094e-01, -4.7023e-01, -2.2844e-01,\n",
      "        -7.4099e-01,  6.8161e-01,  6.2958e-02, -2.4314e-01,  3.0825e-01,\n",
      "        -6.2213e-03,  1.5663e-01, -4.2281e-01,  6.2650e-02,  3.9846e-01,\n",
      "        -1.7244e-01,  1.2154e-01, -1.5552e-01,  6.9881e-02, -5.0767e-01,\n",
      "         7.3178e-02,  6.6642e-01,  8.3370e-01, -1.4294e-01,  2.7906e-01,\n",
      "         7.2443e-02,  4.4941e-01,  3.2904e-01, -6.8149e-02,  3.8360e-01,\n",
      "        -3.4263e-01,  2.4536e-01,  3.4695e-01, -5.3156e-01, -4.1758e-01,\n",
      "        -2.1993e-01,  8.3991e-02,  8.9166e-01, -4.1943e-01,  1.2725e-01,\n",
      "         2.2911e-01,  5.9130e-01,  2.1256e-01, -7.2545e-02,  3.3336e-01,\n",
      "        -6.5255e-01,  2.9253e-01, -1.5527e-01,  2.5367e-01,  6.8592e-01,\n",
      "         8.1754e-01, -1.9586e-01, -1.8196e-01,  3.8215e-02, -5.3986e-01,\n",
      "        -3.7729e-01,  2.2362e-01,  7.8646e-01,  5.5331e-01,  1.2317e+00,\n",
      "        -7.8198e-01, -2.4781e-01, -1.0098e+00,  3.3352e-01, -5.1059e-01,\n",
      "         2.2928e-01,  5.9086e-01,  5.3175e-01,  5.4193e-01,  2.0350e-01,\n",
      "         6.0036e-01, -1.5916e-01, -5.5642e-01,  3.8326e-01,  2.6679e-01,\n",
      "        -3.6887e-01,  1.5012e+00, -2.9788e-01,  1.0739e-01, -3.0869e-01,\n",
      "        -9.7456e-02, -1.8928e+00, -1.2783e+00, -5.6111e-01, -5.2898e-02,\n",
      "         1.2139e-01,  1.7606e-01,  3.0923e-01, -1.9326e-01,  3.9484e-01,\n",
      "        -1.5058e-01, -4.7980e-03,  6.0716e-01,  1.9312e-01, -8.4253e-01,\n",
      "        -1.5631e-01, -4.8636e-01, -3.5978e-01, -6.0869e-01,  6.7241e-01,\n",
      "        -3.0830e-01,  4.5944e-01, -4.5105e-01,  6.6979e-01,  4.4634e-01,\n",
      "        -3.1698e-02,  5.7910e-01,  7.2013e-01, -2.3601e-01, -8.9587e-02,\n",
      "        -2.1428e-01, -4.5907e-01, -1.5606e-01, -1.0682e+00,  3.3689e-01,\n",
      "         3.9914e-01, -4.0523e-01, -7.2061e-01,  5.0093e-01, -5.8293e-01,\n",
      "         3.3036e-01, -5.5723e-01, -2.9140e-01, -1.5720e-01,  6.2032e-01,\n",
      "         3.3338e-01,  2.2213e-01, -2.9876e-01,  6.5420e-02,  4.5114e-02,\n",
      "        -4.6162e-01,  2.3242e-01,  9.9712e-02, -4.6508e-01,  1.4308e-01,\n",
      "        -4.3757e-01, -5.8069e-01, -7.3894e-02,  4.8986e-01,  9.1258e-02,\n",
      "        -2.0579e-01,  4.2314e-01, -9.4584e-02,  4.5499e-01, -2.3914e-01,\n",
      "         2.8518e-01, -3.3063e-01, -6.7729e-02, -5.4989e-02, -5.0081e-01,\n",
      "         7.7920e-02, -1.2118e-01,  9.0742e-01, -5.0140e-01,  4.5210e-01,\n",
      "        -4.5017e-01, -1.0701e+00, -3.9470e-01, -5.2098e-01,  8.0926e-01,\n",
      "        -2.2286e-01, -4.2602e-02,  6.9901e-01, -4.5089e-01,  4.5160e-01,\n",
      "        -1.3761e+00, -2.3607e-01,  7.0740e-02,  5.4893e-01,  4.0661e-02,\n",
      "         1.7227e-01,  4.5018e-01,  2.6999e-01, -6.2015e-01, -8.1615e-02,\n",
      "        -1.4236e-01,  5.5875e-01,  3.6571e-01, -5.7427e-01,  2.9463e-01,\n",
      "         1.5420e-01, -1.0278e+00, -1.9293e-01, -1.1643e-02,  5.9939e-01,\n",
      "         1.2669e-01, -3.6397e-01, -9.2534e-01])\n",
      "0.13339628279209137\n"
     ]
    }
   ],
   "source": [
    "## this is a handy way of finding the cosine similarity between two tensors\n",
    "# see https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html\n",
    "cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "\n",
    "#you use this as:\n",
    "print(encoded_layers[0,0],encoded_layers[0,1])\n",
    "output=cos(encoded_layers[0,0],encoded_layers[0,1])\n",
    "print(output.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7db7f381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# Predict hidden states features for each layer\n",
    "general = {}\n",
    "for count,line in enumerate(content):\n",
    "    tok_id = tokenizer.convert_tokens_to_ids(line)\n",
    "    tok_tensors = torch.tensor([tok_id])\n",
    "\n",
    "    seg_ids = make_segment_ids(line)\n",
    "    seg_id_tensor = torch.tensor([seg_ids])\n",
    "    with torch.no_grad():\n",
    "        # See the models docstrings for the detail of the inputs\n",
    "        outputs = model(tok_tensors, token_type_ids=seg_id_tensor,output_hidden_states=True)\n",
    "        general[count]= outputs[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90db451b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2274,  0.4094,  0.0772,  ..., -0.2385,  0.1318,  0.8467],\n",
       "        [-0.2207, -0.3819,  0.3551,  ..., -0.0472,  0.6077,  0.0413],\n",
       "        [-0.4680, -0.3002,  0.5258,  ..., -0.3344,  0.3603, -0.8147],\n",
       "        ...,\n",
       "        [-0.0987, -0.6426,  0.5858,  ...,  0.0625,  0.1953,  0.0988],\n",
       "        [-0.6355, -0.8042, -0.6895,  ...,  0.1484,  0.1053,  0.1593],\n",
       "        [ 0.9249,  0.2896, -0.1965,  ...,  0.2133, -0.7632, -0.5811]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45791574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9797599911689758\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "cls_encode = []\n",
    "for n in range(0, len(content)-1):\n",
    "    cls_encode.append(general[n][0])\n",
    "\n",
    "similarity_pairs = cos(cls_encode[0][0], cls_encode[1][0])\n",
    "print(similarity_pairs.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84078eda",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/max/Desktop/NLP/Advanced NLP/Deep Learning - Transformers Week 7/lab8/lab8.ipynb Cell 45'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/max/Desktop/NLP/Advanced%20NLP/Deep%20Learning%20-%20Transformers%20Week%207/lab8/lab8.ipynb#ch0000056?line=0'>1</a>\u001b[0m similarity_pairs\u001b[39m.\u001b[39;49mitem()\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a0bae2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cls_encode) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "worse-calcium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.2274,  0.4094,  0.0772,  ..., -0.2385,  0.1318,  0.8467],\n",
       "         [-0.2207, -0.3819,  0.3551,  ..., -0.0472,  0.6077,  0.0413],\n",
       "         [-0.4680, -0.3002,  0.5258,  ..., -0.3344,  0.3603, -0.8147],\n",
       "         ...,\n",
       "         [-0.0987, -0.6426,  0.5858,  ...,  0.0625,  0.1953,  0.0988],\n",
       "         [-0.6355, -0.8042, -0.6895,  ...,  0.1484,  0.1053,  0.1593],\n",
       "         [ 0.9249,  0.2896, -0.1965,  ...,  0.2133, -0.7632, -0.5811]]),\n",
       " tensor([[-0.3158,  0.1626, -0.1255,  ..., -0.1291,  0.1939,  0.8404],\n",
       "         [-0.2033, -0.4116,  0.0197,  ..., -0.1633,  0.3741,  0.0664],\n",
       "         [-0.2414, -0.6789, -0.0744,  ..., -0.2139,  0.3624,  0.1900],\n",
       "         ...,\n",
       "         [-0.5420, -0.4333,  0.4594,  ...,  0.0946,  0.3543, -0.1484],\n",
       "         [-0.8279, -0.4920,  0.1409,  ...,  0.4001,  0.3636, -0.7939],\n",
       "         [ 0.8484,  0.3025, -0.2978,  ...,  0.1438, -0.8105, -0.6094]]),\n",
       " tensor([[-0.4007,  0.3177,  0.1258,  ..., -0.4352,  0.0867,  0.9271],\n",
       "         [-0.5093, -0.3452,  0.3522,  ..., -0.6003,  0.6673,  0.4936],\n",
       "         [-0.6445, -0.1496,  0.2536,  ..., -0.8231,  0.2906, -0.2120],\n",
       "         ...,\n",
       "         [-0.4391, -0.9595,  0.3976,  ..., -0.2480,  0.4586,  0.5602],\n",
       "         [-0.7295, -0.8098, -0.5890,  ..., -0.1610, -0.0469,  0.3154],\n",
       "         [ 0.9513,  0.2305, -0.2063,  ...,  0.0564, -0.7680, -0.3906]]),\n",
       " tensor([[-0.3415,  0.3211,  0.0110,  ..., -0.1217,  0.1450,  0.8538],\n",
       "         [ 0.0488, -0.2927, -0.0782,  ...,  0.1803,  0.2818,  0.2887],\n",
       "         [-0.1887, -0.5337,  0.1506,  ..., -0.0023, -0.2548,  0.1713],\n",
       "         ...,\n",
       "         [-0.2038, -0.1989, -0.1029,  ...,  0.0687,  0.2972,  0.2128],\n",
       "         [-0.9238, -0.4518,  0.2952,  ...,  0.5784,  0.4535, -0.7464],\n",
       "         [ 0.7376,  0.1369, -0.2421,  ...,  0.0762, -0.8138, -0.3550]]),\n",
       " tensor([[-0.2853,  0.1575,  0.1631,  ..., -0.2328,  0.0742,  0.8326],\n",
       "         [ 0.1102, -0.5120, -0.2126,  ...,  0.0087,  0.0387,  0.4352],\n",
       "         [-0.3413, -0.4106,  0.1168,  ..., -0.1413, -0.4138, -0.2475],\n",
       "         [-0.1652,  0.0130, -0.3500,  ...,  0.1566, -0.5552,  0.8038],\n",
       "         [ 0.0296, -0.4529, -0.2216,  ..., -0.0455,  0.1018,  0.4213],\n",
       "         [ 0.9433,  0.0929, -0.2834,  ...,  0.0987, -0.7822, -0.2994]]),\n",
       " tensor([[-0.2660,  0.1900,  0.0080,  ..., -0.1477,  0.1057,  0.1043],\n",
       "         [-0.4603, -0.4844, -0.3335,  ...,  0.2011,  0.8531, -0.5401],\n",
       "         [-0.3205, -0.2159,  0.4862,  ..., -0.0934,  0.1916, -1.0124],\n",
       "         [-0.4355, -0.5625, -0.3667,  ...,  0.3854,  0.5478, -0.1662],\n",
       "         [ 0.9923,  0.3134, -0.3585,  ..., -0.1562, -0.7963, -0.4315]]),\n",
       " tensor([[-3.6279e-01,  1.6045e-01, -1.1321e-01,  ..., -1.6836e-01,\n",
       "           1.2154e-01,  5.9421e-04],\n",
       "         [-6.7528e-01, -4.3463e-01, -3.1767e-01,  ..., -2.6851e-01,\n",
       "           4.2054e-01, -4.3054e-01],\n",
       "         [-6.9039e-01, -1.0293e-01,  2.5615e-01,  ..., -4.6051e-01,\n",
       "           3.9885e-01, -8.1635e-01],\n",
       "         [-7.2936e-01, -6.0943e-01, -3.9849e-01,  ...,  5.9507e-01,\n",
       "           4.9560e-01, -2.2364e-01],\n",
       "         [ 8.7700e-01,  1.3955e-01, -2.6677e-01,  ..., -1.2996e-01,\n",
       "          -8.0844e-01, -4.3984e-01]]),\n",
       " tensor([[-0.3644,  0.7079, -0.0953,  ..., -0.0552,  0.1503,  0.5564],\n",
       "         [-0.2745, -0.0646, -0.3135,  ...,  0.2387,  0.8448, -0.4523],\n",
       "         [-0.6834, -0.4367,  0.1995,  ..., -0.1953,  0.4551, -1.1127],\n",
       "         ...,\n",
       "         [ 0.2910,  0.7840,  0.1179,  ..., -0.0895, -0.0990,  0.1015],\n",
       "         [-0.2432,  0.0359, -0.6851,  ...,  0.2813,  0.5164,  0.2918],\n",
       "         [ 0.8416,  0.2622, -0.3584,  ...,  0.2572, -0.6163, -0.4332]]),\n",
       " tensor([[-0.4498,  0.2142,  0.1608,  ..., -0.5457,  0.1484,  0.8255],\n",
       "         [-0.6380, -0.4126, -0.1470,  ..., -0.6275,  0.6269,  0.2430],\n",
       "         [-0.2510, -0.0628,  0.3476,  ..., -0.6808,  0.3222, -0.1230],\n",
       "         ...,\n",
       "         [-0.4101, -0.9345,  0.1189,  ..., -0.2189,  0.2575,  0.5923],\n",
       "         [-0.6858, -0.8015, -0.6883,  ..., -0.4392, -0.2794,  0.2365],\n",
       "         [ 1.0064,  0.1875, -0.2004,  ...,  0.1217, -0.5156, -0.3212]]),\n",
       " tensor([[-0.2808,  0.4797,  0.0188,  ..., -0.1435,  0.4110,  0.3105],\n",
       "         [-0.3450, -0.5354, -0.2231,  ..., -0.0972,  1.1263, -0.5729],\n",
       "         [-0.2814,  0.0031,  0.7893,  ..., -0.3824,  0.7165, -0.9618],\n",
       "         ...,\n",
       "         [-0.1528, -0.4251,  0.3676,  ...,  0.0038, -0.1426,  0.1334],\n",
       "         [-0.6514, -0.1772, -0.1677,  ..., -0.2444,  0.3449, -0.1197],\n",
       "         [ 0.8292,  0.0989, -0.3274,  ...,  0.0605, -0.5008, -0.3876]]),\n",
       " tensor([[-0.2294,  0.3338, -0.0435,  ..., -0.2204,  0.1469,  0.7773],\n",
       "         [-0.3989, -0.4763,  0.0404,  ..., -0.0740,  0.7071,  0.1449],\n",
       "         [-0.5743, -0.3694,  0.4627,  ..., -0.1719,  0.3139, -0.6625],\n",
       "         ...,\n",
       "         [-0.0035, -0.6706,  0.3692,  ...,  0.1078,  0.4032,  0.1492],\n",
       "         [-0.4642, -0.8797, -0.6300,  ...,  0.3093,  0.0810,  0.3300],\n",
       "         [ 0.8835,  0.1763, -0.2806,  ...,  0.1915, -0.7542, -0.4860]]),\n",
       " tensor([[-0.1903,  0.1489, -0.0137,  ..., -0.1295,  0.1258,  0.6903],\n",
       "         [-0.1157, -0.8259, -0.4091,  ..., -0.0751,  0.3136,  0.0961],\n",
       "         [-0.2639, -0.8498, -0.1038,  ..., -0.2116,  0.0587,  0.4886],\n",
       "         ...,\n",
       "         [-0.5653, -0.5675,  0.1975,  ...,  0.0956,  0.5241,  0.1063],\n",
       "         [-0.9508, -0.5814,  0.0817,  ...,  0.5653,  0.2598, -0.7101],\n",
       "         [ 0.8782,  0.2126, -0.2903,  ...,  0.0341, -0.7991, -0.5755]]),\n",
       " tensor([[-0.4398,  0.1950, -0.0411,  ..., -0.0811,  0.1150,  0.9023],\n",
       "         [-0.7840, -0.3593,  0.1167,  ..., -0.0194,  0.8741,  0.2446],\n",
       "         [-0.7234, -0.4732,  0.4191,  ..., -0.0219,  0.5239, -0.4405],\n",
       "         ...,\n",
       "         [-0.2001, -0.2932,  0.1978,  ...,  0.1257,  0.0373,  0.4865],\n",
       "         [-0.4522, -0.7868, -0.5241,  ...,  0.2079, -0.0272,  0.5859],\n",
       "         [ 0.8254,  0.1575, -0.3784,  ...,  0.0866, -0.7515, -0.4177]]),\n",
       " tensor([[-0.3084,  0.1225,  0.0390,  ..., -0.0052,  0.1883,  0.9071],\n",
       "         [-0.3081, -0.5247,  0.0232,  ...,  0.0988,  0.6607,  0.3920],\n",
       "         [-0.2618, -0.8984,  0.0362,  ..., -0.1156,  0.0650,  0.6446],\n",
       "         ...,\n",
       "         [-0.3309, -0.5062,  0.0852,  ...,  0.1246,  0.3389,  0.3520],\n",
       "         [-0.9391, -0.7863,  0.1923,  ...,  0.8031,  0.4060, -0.5895],\n",
       "         [ 0.7194,  0.1693, -0.3464,  ...,  0.0336, -0.8412, -0.4505]]),\n",
       " tensor([[-0.4853,  0.1806,  0.0879,  ..., -0.5581,  0.0883,  0.8254],\n",
       "         [-0.6699, -0.3103, -0.1019,  ..., -0.6204,  0.5416,  0.3252],\n",
       "         [-0.2172, -0.5234,  0.0418,  ..., -0.7033,  0.0623, -0.0204],\n",
       "         ...,\n",
       "         [-0.4476, -0.8718,  0.2010,  ..., -0.2028,  0.1745,  0.6307],\n",
       "         [-0.6381, -0.7735, -0.6540,  ..., -0.5140, -0.3507,  0.2910],\n",
       "         [ 0.9771,  0.1487, -0.2447,  ...,  0.0994, -0.5562, -0.3064]]),\n",
       " tensor([[-0.3089,  0.2329,  0.0590,  ..., -0.2821,  0.1138,  0.8352],\n",
       "         [-0.3609, -0.6586,  0.2891,  ..., -0.3428,  0.5210,  0.1462],\n",
       "         [-0.7240, -1.3076,  0.2415,  ..., -0.5348,  0.2304, -0.2978],\n",
       "         ...,\n",
       "         [-0.2710, -0.7281,  0.5799,  ..., -0.2066,  0.3707,  0.1657],\n",
       "         [-0.8867, -0.9658, -0.6807,  ...,  0.0161,  0.1968,  0.1144],\n",
       "         [ 0.9439,  0.1257, -0.3117,  ...,  0.1860, -0.7778, -0.6051]]),\n",
       " tensor([[-0.4049,  0.5517,  0.0051,  ..., -0.3792,  0.1789,  0.5086],\n",
       "         [-0.3497, -0.3097, -0.0248,  ..., -0.6087,  0.3304, -0.2367],\n",
       "         [-0.4576, -0.1182,  0.2049,  ..., -1.0335,  0.2856, -0.7922],\n",
       "         ...,\n",
       "         [-0.1034,  0.0034,  0.2429,  ..., -0.1190,  0.0421, -0.2130],\n",
       "         [-0.7531, -0.4696, -0.3537,  ...,  0.2467,  0.7539,  0.2236],\n",
       "         [ 0.9173,  0.1051, -0.4106,  ..., -0.0913, -0.6613, -0.4296]]),\n",
       " tensor([[-0.4440,  0.5230,  0.0577,  ..., -0.3927,  0.1785,  0.4637],\n",
       "         [-0.3942, -0.3497, -0.2096,  ..., -0.5211,  0.4928, -0.4303],\n",
       "         [-0.1773, -0.1433, -0.0604,  ..., -0.6719,  0.0781, -0.4089],\n",
       "         ...,\n",
       "         [-0.2159,  0.1113,  0.1933,  ..., -0.1091, -0.0317, -0.2447],\n",
       "         [-0.8603, -0.3025, -0.3742,  ...,  0.2825,  0.7423,  0.2531],\n",
       "         [ 0.9486,  0.1340, -0.3527,  ...,  0.0030, -0.5663, -0.4444]]),\n",
       " tensor([[-0.3909,  0.4880,  0.0556,  ..., -0.4353,  0.2656,  0.5282],\n",
       "         [-0.3662, -0.4477, -0.3617,  ..., -0.5973,  0.6938, -0.3892],\n",
       "         [-0.1638,  0.2869,  0.3258,  ..., -0.6761,  0.3363, -0.4411],\n",
       "         ...,\n",
       "         [-0.1907,  0.0287,  0.1211,  ..., -0.1081,  0.0078, -0.2092],\n",
       "         [-0.8576, -0.3630, -0.3938,  ...,  0.2991,  0.7384,  0.2704],\n",
       "         [ 1.0079,  0.1658, -0.3550,  ...,  0.0609, -0.4746, -0.4203]]),\n",
       " tensor([[-0.3643,  0.5162, -0.0440,  ..., -0.2371,  0.1965,  0.5308],\n",
       "         [-0.3495, -0.5742,  0.0510,  ..., -0.3074,  0.5015, -0.4658],\n",
       "         [-0.3839, -1.1796,  0.3062,  ..., -0.4542,  0.2033, -0.5381],\n",
       "         ...,\n",
       "         [-0.2571, -0.0829,  0.3340,  ..., -0.0386,  0.1080, -0.4606],\n",
       "         [-0.8917, -0.6214, -0.2961,  ...,  0.3672,  0.7951,  0.1579],\n",
       "         [ 0.9534,  0.0454, -0.4349,  ..., -0.0370, -0.7288, -0.5269]]),\n",
       " tensor([[-0.3212,  0.5744, -0.0807,  ..., -0.2839,  0.2435,  0.4981],\n",
       "         [-0.1951, -0.3960, -0.0444,  ..., -0.2103,  0.5586, -0.4705],\n",
       "         [-0.1391, -0.1270,  0.6231,  ..., -0.5012,  0.2979, -1.1344],\n",
       "         ...,\n",
       "         [-0.1886,  0.0750,  0.2858,  ...,  0.0467,  0.1217, -0.2785],\n",
       "         [-0.7532, -0.5875, -0.3567,  ...,  0.4426,  0.7429,  0.0940],\n",
       "         [ 0.9331,  0.1273, -0.3749,  ..., -0.0509, -0.6537, -0.4578]]),\n",
       " tensor([[-6.9806e-02,  4.9586e-01,  6.4192e-02,  ..., -1.1355e-01,\n",
       "           2.4836e-01,  3.7190e-01],\n",
       "         [-1.0594e-01, -2.6768e-01, -2.4200e-01,  ..., -1.6743e-03,\n",
       "           7.2645e-01, -4.4216e-01],\n",
       "         [-3.1333e-01, -3.3136e-01,  6.1988e-01,  ..., -3.4438e-01,\n",
       "           3.0455e-01, -8.0950e-01],\n",
       "         ...,\n",
       "         [ 8.6273e-02, -6.3534e-01,  5.5810e-01,  ..., -4.1849e-02,\n",
       "           2.6461e-05, -2.0527e-01],\n",
       "         [-4.8532e-01, -7.0099e-01, -5.5316e-01,  ...,  1.8917e-01,\n",
       "          -2.8877e-01, -2.6604e-02],\n",
       "         [ 7.7478e-01,  3.0807e-01, -2.6874e-01,  ..., -1.5371e-01,\n",
       "          -6.7699e-01, -4.3428e-01]]),\n",
       " tensor([[-0.4926,  0.2883, -0.2304,  ...,  0.0012,  0.5075,  0.5690],\n",
       "         [-0.4613, -0.1190, -0.7146,  ..., -0.1183,  1.0048, -0.5340],\n",
       "         [-0.3765, -0.3987,  0.2827,  ..., -0.2534,  0.4094, -0.7315],\n",
       "         ...,\n",
       "         [-0.1585, -0.4261, -0.0476,  ...,  0.1343,  0.3607, -0.0513],\n",
       "         [-0.1995, -0.4844, -0.5487,  ...,  0.3108, -0.4574,  0.1162],\n",
       "         [ 0.5421,  0.1923, -0.4655,  ..., -0.0126, -0.6648, -0.4587]]),\n",
       " tensor([[-0.0787,  0.3754, -0.1295,  ..., -0.1727,  0.3161,  0.3746],\n",
       "         [-0.2161,  0.4058,  0.3098,  ...,  0.0479,  0.7380, -0.2129],\n",
       "         [-0.0761,  0.5838, -0.0659,  ...,  0.0898,  0.5075, -0.7635],\n",
       "         [ 0.0923,  0.1249,  0.2589,  ..., -0.1605,  0.3295, -0.1628],\n",
       "         [-0.5974,  0.0224,  0.2412,  ...,  0.3749,  0.2275, -0.3493],\n",
       "         [ 0.8578,  0.2506, -0.0445,  ...,  0.0971, -0.8121, -0.2928]]),\n",
       " tensor([[ 0.0251,  0.4913, -0.1633,  ..., -0.0091,  0.4783,  0.5032],\n",
       "         [ 0.0972,  0.3365,  0.0027,  ...,  0.2736,  0.8177,  0.0919],\n",
       "         [ 0.0751, -0.2625,  0.6169,  ..., -0.3722,  0.3564, -0.7145],\n",
       "         ...,\n",
       "         [-0.3302,  0.1106,  0.2926,  ..., -0.2753,  0.2917,  0.6443],\n",
       "         [-0.7522, -0.5680, -0.3886,  ...,  0.0787,  0.2874,  0.0023],\n",
       "         [ 0.9136,  0.1978, -0.2868,  ...,  0.0011, -0.7645, -0.3094]]),\n",
       " tensor([[-5.1487e-02,  2.9544e-01, -3.5177e-01,  ..., -1.4157e-01,\n",
       "           2.7627e-01,  3.3091e-01],\n",
       "         [-1.2741e-01,  8.0756e-02, -4.6489e-01,  ...,  5.6725e-02,\n",
       "           5.9103e-01, -5.4720e-01],\n",
       "         [-5.5014e-04, -4.9426e-02, -1.9428e-01,  ...,  7.7054e-02,\n",
       "           1.2964e-01, -1.6760e-01],\n",
       "         ...,\n",
       "         [-1.3790e-01, -1.8781e-01,  4.0009e-01,  ..., -6.7827e-02,\n",
       "          -5.4403e-02,  4.8118e-01],\n",
       "         [-1.1189e-01, -4.9280e-01, -2.6717e-01,  ..., -3.4615e-01,\n",
       "          -7.8325e-02,  1.7802e-01],\n",
       "         [ 7.7646e-01,  1.4424e-01, -4.5575e-01,  ..., -6.8048e-02,\n",
       "          -5.6302e-01, -3.2866e-01]]),\n",
       " tensor([[ 0.1257,  0.2336, -0.2640,  ..., -0.0370,  0.4333,  0.5687],\n",
       "         [ 0.0602,  0.1682, -0.3525,  ..., -0.3539,  0.8506, -0.4406],\n",
       "         [ 0.0173,  0.0137, -0.0202,  ..., -0.2169,  0.3978,  0.4334],\n",
       "         ...,\n",
       "         [-0.1855, -0.1535,  0.5054,  ..., -0.2124, -0.0590,  0.7670],\n",
       "         [-0.1710, -0.3413, -0.2439,  ..., -0.2768, -0.1547,  0.1544],\n",
       "         [ 0.7253,  0.2588, -0.3435,  ...,  0.0570, -0.6956, -0.3678]]),\n",
       " tensor([[ 0.1284,  0.4182, -0.0786,  ..., -0.0703,  0.2515,  0.3541],\n",
       "         [ 0.1635,  0.3378, -0.1412,  ...,  0.0652,  0.7866,  0.1514],\n",
       "         [ 0.4558,  0.1512, -0.1050,  ..., -0.1641,  0.2535, -0.6997],\n",
       "         ...,\n",
       "         [-0.0969, -0.2088,  0.0712,  ..., -0.1227, -0.1407,  0.3662],\n",
       "         [-0.7555, -0.6249, -0.6542,  ..., -0.0103, -0.1615, -0.0394],\n",
       "         [ 0.6492,  0.3315, -0.1330,  ...,  0.0156, -0.4414, -0.2994]]),\n",
       " tensor([[ 0.1359,  0.3505, -0.0642,  ...,  0.0024,  0.2037,  0.3517],\n",
       "         [ 0.1812,  0.4893,  0.0621,  ..., -0.0166,  0.5643,  0.0808],\n",
       "         [ 0.2094,  0.1820,  0.1194,  ..., -0.0982,  0.0787, -0.1471],\n",
       "         ...,\n",
       "         [-0.3465, -0.1087,  0.2770,  ..., -0.3253, -0.0262,  0.5650],\n",
       "         [-0.8212, -0.6152, -0.5908,  ..., -0.0531, -0.1505,  0.1494],\n",
       "         [ 0.6498,  0.4729, -0.2938,  ..., -0.1990, -0.5735, -0.3205]]),\n",
       " tensor([[-0.0041,  0.2722,  0.2335,  ..., -0.0446,  0.1753,  0.2616],\n",
       "         [ 0.2770,  0.3557,  0.4319,  ..., -0.4870,  0.8288, -0.0525],\n",
       "         [ 0.7999,  0.8107,  0.2702,  ..., -0.3141,  0.1508, -0.2549],\n",
       "         ...,\n",
       "         [ 0.1137, -0.6065,  0.3740,  ..., -0.1068,  0.2788, -0.0456],\n",
       "         [-0.9415, -0.5761, -0.4080,  ...,  0.3460, -0.1035, -0.4209],\n",
       "         [ 0.6531,  0.3550, -0.1503,  ..., -0.2107, -0.6443, -0.4121]]),\n",
       " tensor([[ 0.1195,  0.2150, -0.1501,  ..., -0.1306,  0.2007,  0.2696],\n",
       "         [ 0.5580,  0.0653, -0.0429,  ..., -0.2472,  0.6320, -0.0685],\n",
       "         [-0.2840,  0.3707,  0.5205,  ..., -0.3050,  0.0833, -0.8502],\n",
       "         [ 0.2354,  0.0821,  0.5594,  ...,  0.0028,  0.3631, -0.1004],\n",
       "         [-0.6389, -0.0946,  0.1169,  ...,  0.2841,  0.2433, -0.3430],\n",
       "         [ 0.9136,  0.2871, -0.0031,  ..., -0.1187, -0.7862, -0.3251]]),\n",
       " tensor([[-0.1191,  0.1546, -0.2884,  ..., -0.0727,  0.3396,  0.5881],\n",
       "         [-0.0869,  0.4901, -0.3740,  ..., -0.2781,  0.4166, -0.6654],\n",
       "         [-0.0467,  0.6241, -0.6602,  ...,  0.1378,  0.4097, -0.0806],\n",
       "         ...,\n",
       "         [ 0.1286,  0.0257,  0.4628,  ..., -0.2447,  0.0691,  0.0217],\n",
       "         [-0.7426, -0.0923, -0.0170,  ...,  0.2495,  0.1668,  0.0524],\n",
       "         [ 0.5825,  0.1851, -0.1714,  ...,  0.2495, -0.7791, -0.1759]]),\n",
       " tensor([[ 4.8050e-02,  2.4465e-01, -1.6767e-01,  ..., -2.2303e-01,\n",
       "           3.7816e-01,  3.1967e-01],\n",
       "         [ 1.0529e-01,  3.3333e-01,  1.5993e-01,  ..., -1.2187e-01,\n",
       "           5.2286e-01, -5.5546e-01],\n",
       "         [ 3.0993e-01, -3.4134e-01,  6.9903e-02,  ...,  2.2790e-01,\n",
       "           3.9878e-01, -2.5684e-01],\n",
       "         [ 2.2931e-01, -1.7123e-01,  4.3907e-01,  ..., -2.0836e-01,\n",
       "           5.0322e-01, -4.7432e-02],\n",
       "         [-5.0225e-01, -1.8467e-01,  2.3877e-01,  ...,  4.8138e-01,\n",
       "           4.2821e-01, -2.1744e-01],\n",
       "         [ 8.5287e-01,  2.1542e-01, -7.8509e-02,  ...,  2.9726e-04,\n",
       "          -6.9778e-01, -3.0050e-01]]),\n",
       " tensor([[ 0.0643,  0.3948,  0.0159,  ..., -0.1435,  0.2300,  0.3508],\n",
       "         [ 0.2329,  0.5415,  0.1213,  ..., -0.0947,  0.7533,  0.0864],\n",
       "         [ 0.4056,  0.3372, -0.0378,  ..., -0.2310,  0.2775, -0.6062],\n",
       "         ...,\n",
       "         [-0.1189,  0.0035, -0.1877,  ..., -0.2448, -0.2291,  0.3730],\n",
       "         [-0.6691, -0.5085, -0.6670,  ..., -0.1064, -0.2301, -0.2277],\n",
       "         [ 0.5546,  0.4119, -0.0775,  ...,  0.0015, -0.3918, -0.2541]])]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2 All pair similarities\n",
    "#3 Select top 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-joint",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "a) Repeat exercise 2 but use the centroid of all of the output embeddings as the representation of a sentence.\n",
    "\n",
    "b) Experiment with using different pooling layers from the hidden state embeddings.  Typically, using the penultimate layer (-2) is felt to be optimal as it is far enough away from the original uncontextualised word embeddings but also not too close to the output predictions.  See here for a discussion: https://github.com/hanxiao/bert-as-service#q-what-are-the-available-pooling-strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-debate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-moses",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims=run(sentences,poolinglayer=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sims=run(sentences,method=\"cls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-sauce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-contributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sims[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-facility",
   "metadata": {},
   "outputs": [],
   "source": [
    "interested=[2,1,21,7,8,22,3,25]\n",
    "for i in interested:\n",
    "    print(sentences[i],sims[0][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-dodge",
   "metadata": {},
   "source": [
    "### Extension 1\n",
    "The MRPC.zip file contains a training, dev and test split for the Microsoft Research paraphrase corpus.  In this corpus the quality '1' indicates that the 2 sentences are considered to be paraphrases and '0' indicates that they are not.\n",
    "\n",
    "Can you build a classifier on top of the BERT pre-trained model, trained on the training split of MRPC, which predicts whether 2 sentences are paraphrases or not?\n",
    "\n",
    "Note this does not require you to fine-tune the BERT model.  You can use outputs from BERT as input to your separate classifier.  I would suggest a single neural layer which uses the representation from exercise 2 or 3 as input, built using scikit-learn or torch.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-portuguese",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
